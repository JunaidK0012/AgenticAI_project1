{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2332a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langchain_core.tools import tool\n",
    "from typing import TypedDict,Annotated\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "from langchain_core.messages import HumanMessage,BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage,ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import Command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf7736f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce21f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web search tool \n",
    "from langchain_tavily import TavilySearch\n",
    "web_search_tool = TavilySearch(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9628b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File management tool\n",
    "from langchain_community.agent_toolkits import FileManagementToolkit\n",
    "\n",
    "working_directory = './files'\n",
    "\n",
    "file_management_tools =FileManagementToolkit(\n",
    "    root_dir=str(working_directory),\n",
    "    selected_tools=[\"read_file\", \"write_file\", \"list_directory\"]\n",
    ").get_tools()\n",
    "\n",
    "\n",
    "read_tool, write_tool, list_tool = file_management_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b431f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool,tool\n",
    "from langgraph.prebuilt.interrupt import HumanInterruptConfig,HumanInterrupt\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.types import interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05944243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_human_in_the_loop(toolhitl,interrupt_config: HumanInterruptConfig = None) -> BaseTool:\n",
    "    \"\"\"Wrap a tool to support human-in-the-loop review.\"\"\"\n",
    "\n",
    "    if not isinstance(toolhitl, BaseTool):\n",
    "        toolhitl = tool(toolhitl)\n",
    "\n",
    "    if interrupt_config is None:\n",
    "        interrupt_config = {\n",
    "            \"allow_accept\":True,\n",
    "            \"allow_edit\": True,\n",
    "            \"allow_respond\":True\n",
    "        }\n",
    "\n",
    "    @tool(toolhitl.name,description=toolhitl.description,args_schema=toolhitl.args_schema)\n",
    "    def call_tool_with_interrupt(config: RunnableConfig, **tool_input):\n",
    "        request: HumanInterrupt = {\n",
    "            'action_request':{\n",
    "                \"action\":toolhitl.name,\n",
    "                \"args\":tool_input\n",
    "            },\n",
    "            \"config\":interrupt_config,\n",
    "            \"description\": \"Please review the tool call\"\n",
    "        }\n",
    "\n",
    "        response = interrupt([request])[0]\n",
    "\n",
    "        # approve the tool call\n",
    "        if response[\"type\"] == \"accept\":\n",
    "            tool_response = toolhitl.invoke(tool_input, config)\n",
    "        # update tool call args\n",
    "        elif response[\"type\"] == \"edit\":\n",
    "            tool_input = response[\"args\"][\"args\"]\n",
    "            tool_response = toolhitl.invoke(tool_input, config)\n",
    "        # respond to the LLM with user feedback\n",
    "        elif response[\"type\"] == \"response\":\n",
    "            user_feedback = response[\"args\"]\n",
    "            tool_response = user_feedback\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n",
    "\n",
    "        return tool_response\n",
    "    \n",
    "    return call_tool_with_interrupt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d45ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arxiv\n",
    "import arxiv\n",
    "\n",
    "@tool(\"arxiv_search\")\n",
    "def arxiv_search(query: str,max_results: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Searches arXiv for papers matching the query.\n",
    "    - query: keywords, authors or title\n",
    "    - max_results: number of papers to return\n",
    "    \"\"\"\n",
    "    try:\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        papers = []\n",
    "        for result in search.results():\n",
    "            pdf_url = result.pdf_url if hasattr(result,\"pdf_url\") else result.entry_id.replace(\"abs\",\"pdf\")\n",
    "            papers.append(\n",
    "                f\"Title: {result.title}\\n\"\n",
    "                f\"Authors: {','.join(a.name for a in result.authors)}\\n\"\n",
    "                f'Published: {result.published.date()}\\n'\n",
    "                f\"Abstract: {result.summary.strip()}\\n\"\n",
    "                f\"Link: {result.entry_id}\\n\"\n",
    "                f\"PDF: {pdf_url}\\n\"\n",
    "                + \"-\"*80\n",
    "\n",
    "            )\n",
    "        if not papers:\n",
    "            return f\"No results found for '{query}\"\n",
    "        return \"\\n\".join(papers)\n",
    "    except Exception as e:\n",
    "        return f\"Error during arXiv search: {e}\"\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bee6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JunaidKhan\\AppData\\Local\\Temp\\ipykernel_11032\\2627632007.py:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = arxiv_search(\"Quantum Machine Learning\")\n",
      "C:\\Users\\JunaidKhan\\AppData\\Local\\Temp\\ipykernel_11032\\2323917343.py:18: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    }
   ],
   "source": [
    "result = arxiv_search(\"Quantum Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be0cb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikipedia\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(load_all_available_meta=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa930a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#youtube\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "youtube_tool = YouTubeSearchTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b957f552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['https://www.youtube.com/shorts/dMBVuRYzAKs', 'https://www.youtube.com/watch?v=4FFspU4riHk&pp=ygUHQ2FtcHVzWA%3D%3D']\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_tool.run(\"CampusX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41788055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.tools import Tool,tool\n",
    "python_repl = PythonREPL()\n",
    "# You can create the tool to pass to an agent\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class PythonREPLInput(BaseModel):\n",
    "    code: str\n",
    "\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Input should be Python code as a string.\",\n",
    "    args_schema=PythonREPLInput,\n",
    "    func=lambda code: python_repl.run(code)  # map `code` -> REPL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec1daf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "#from langgraph.store.sqlite import SqliteStore\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.config import get_store\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Optional\n",
    "\n",
    "store = InMemoryStore() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "325ae933",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_user_info(config: RunnableConfig) -> str:\n",
    "    \"\"\"Look up user info.\"\"\"\n",
    "    store = get_store()\n",
    "    user_id = config['configurable'].get(\"user_id\")\n",
    "    user_info = store.get((\"users\",),user_id)\n",
    "    return str(user_info.value) if user_info else \"Unknown user\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "    \n",
    "\n",
    "@tool \n",
    "def save_user_info(user_info: Dict[str, Any], config: RunnableConfig) -> str:\n",
    "    \"\"\"\n",
    "    Save arbitrary user info as key-value pairs.\n",
    "    Always pass `user_info` as a JSON object (not a string).\n",
    "    Example: {\"name\": \"John\", \"age\": 30}\n",
    "    \"\"\"\n",
    "    store = get_store()\n",
    "    user_id = config['configurable'].get(\"user_id\")\n",
    "    store.put((\"users\",), user_id, user_info)\n",
    "    return \"Successfully saved user info \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "243a1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "tools = [add_human_in_the_loop(repl_tool),arxiv_search,wikipedia_tool,youtube_tool,read_tool, add_human_in_the_loop(write_tool), list_tool,web_search_tool,save_user_info,get_user_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be4a34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm\n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-2.5-flash')\n",
    "llm_with_tools = llm.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93797cc",
   "metadata": {},
   "source": [
    "GRAPH BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52228cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages : Annotated[list[BaseMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17e79e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "CURRENT_TIME_IST = datetime.now().astimezone().strftime(\"%Y-%m-%d %H:%M %Z\")\n",
    "\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an intelligent reasoning agent that helps users by combining natural conversation \n",
    "with external tools when needed.\n",
    "\n",
    "Current date/time: {CURRENT_TIME_IST}\n",
    "\n",
    "CORE BEHAVIOR\n",
    "- Think step by step privately. Do NOT reveal chain-of-thought; only show final answers and short explanations.\n",
    "- Prefer external tools for time-sensitive, date-related, or “latest” queries. If a tool is available and relevant, USE IT.\n",
    "- When any tool is used, BASE YOUR FINAL ANSWER ONLY ON THE TOOL OBSERVATIONS. Do not mix in prior knowledge that conflicts with tools.\n",
    "- If tools fail or are insufficient, say so and propose the next best tool/action.\n",
    "\n",
    "TEMPORAL & FRESHNESS RULES\n",
    "- If the user mentions a year/date (e.g., 2025) or asks for \"latest/current/today/this year\", you MUST consult a web/search tool first.\n",
    "- Do NOT claim “this hasn’t happened yet” or “it’s a future date” unless you have explicitly checked the current date AND the tool results confirm it (e.g., no results or explicit statements about future scheduling).\n",
    "\n",
    "\n",
    "### Reasoning Framework\n",
    "Follow the ReAct reasoning loop:\n",
    "1. **Thought** — explain what you are thinking or planning.\n",
    "2. **Action** — choose the correct tool to use.\n",
    "3. **Action Input** — provide the exact structured input for the tool.\n",
    "4. **Observation** — read the tool's result and update your reasoning.\n",
    "\n",
    "Repeat this loop until you can confidently respond to the user.\n",
    "\n",
    "### Style & Tone\n",
    "- Be concise but complete.\n",
    "- Use plain language that non-technical users can understand.\n",
    "- If user input is ambiguous, ask clarifying questions before acting.\n",
    "- Never hallucinate tool outputs. If unsure, say so.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa66dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner_node(state):\n",
    "    user_query = state['messages']\n",
    "\n",
    "    planner_prompt = ChatPromptTemplate([\n",
    "        ('system',system_prompt),\n",
    "        MessagesPlaceholder(variable_name='messages')\n",
    "    ])\n",
    "\n",
    "    planner = planner_prompt | llm_with_tools\n",
    "    result = planner.invoke({'messages': state['messages']})\n",
    "\n",
    "\n",
    "    return ({'messages':result}) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "772b7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)\n",
    "checkpointer = InMemorySaver()\n",
    "tool_node = ToolNode(tools)\n",
    "graph.add_node('planner_node',planner_node)\n",
    "graph.add_node('tools',tool_node)\n",
    "\n",
    "graph.add_edge(START,'planner_node')\n",
    "graph.add_conditional_edges('planner_node',tools_condition)\n",
    "graph.add_edge('tools','planner_node')\n",
    "graph = graph.compile(checkpointer=checkpointer,store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abe89fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d733a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"I like to play Basketball\")]\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"t2\" ,\"user_id\": \"user_23\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23044bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "chunk =  graph.invoke(\n",
    "    initial_state,\n",
    "    config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "115956c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hii i am john , please explain LLM in 5 sentences ', additional_kwargs={}, response_metadata={}, id='d7c60e5c-2865-4f03-8e42-7abbfc563e0a'),\n",
       "  AIMessage(content='Hi John! LLM stands for Large Language Model. These are powerful AI models trained on vast amounts of text data. They learn to understand, generate, and predict human language. LLMs can perform various tasks like answering questions, writing different kinds of creative content, and translating languages. They are at the core of many modern AI applications.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--f1d751d1-418b-405b-84b3-6e1540b4de1f-0', usage_metadata={'input_tokens': 2195, 'output_tokens': 113, 'total_tokens': 2308, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 45}}),\n",
       "  HumanMessage(content='hii i am john , remember my name ', additional_kwargs={}, response_metadata={}, id='60fc828f-b9b9-4626-a734-2cd83002c002'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'save_user_info', 'arguments': '{\"user_info\": \"{\\\\\"name\\\\\": \\\\\"John\\\\\"}\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--a6890c57-ad76-43bd-a99f-399b41319f8f-0', tool_calls=[{'name': 'save_user_info', 'args': {'user_info': '{\"name\": \"John\"}'}, 'id': '05bf5327-85fd-4be8-aca6-cd256ef2c6fd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2274, 'output_tokens': 66, 'total_tokens': 2340, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 41}}),\n",
       "  ToolMessage(content='Error: 1 validation error for save_user_info\\nuser_info\\n  Input should be a valid dictionary [type=dict_type, input_value=\\'{\"name\": \"John\"}\\', input_type=str]\\n    For further information visit https://errors.pydantic.dev/2.11/v/dict_type\\n Please fix your mistakes.', name='save_user_info', id='a7f9e8fd-2571-4545-8f36-6b69b5ba5a5d', tool_call_id='05bf5327-85fd-4be8-aca6-cd256ef2c6fd', status='error'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'save_user_info', 'arguments': '{\"user_info\": {\"name\": \"John\"}}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--37b588cf-7a76-481d-bec8-f70e626eda80-0', tool_calls=[{'name': 'save_user_info', 'args': {'user_info': {'name': 'John'}}, 'id': 'a3f20b93-6056-4a78-baae-7f181cf4a887', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2400, 'output_tokens': 74, 'total_tokens': 2474, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 51}}),\n",
       "  ToolMessage(content='Successfully saved user info ', name='save_user_info', id='393cbccd-2587-4ed9-90d0-c77c843df1d4', tool_call_id='a3f20b93-6056-4a78-baae-7f181cf4a887'),\n",
       "  AIMessage(content=\"Got it, John! I've saved your name.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--36cbb388-100c-4d89-a984-6abbff08af37-0', usage_metadata={'input_tokens': 2444, 'output_tokens': 70, 'total_tokens': 2514, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 58}}),\n",
       "  HumanMessage(content='I like to play Basketball', additional_kwargs={}, response_metadata={}, id='10815cef-c006-404e-a305-98f6737dbb48'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_user_info', 'arguments': '{}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--5d4f18f8-14fc-4729-99f3-0563408a99e4-0', tool_calls=[{'name': 'get_user_info', 'args': {}, 'id': '9a1b5a89-8d22-4fef-aeab-a42f9950a2ab', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2463, 'output_tokens': 79, 'total_tokens': 2542, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 67}}),\n",
       "  ToolMessage(content=\"{'name': 'John'}\", name='get_user_info', id='8fa8a457-f5f4-438e-b061-0d5b70f3089f', tool_call_id='9a1b5a89-8d22-4fef-aeab-a42f9950a2ab'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'save_user_info', 'arguments': '{\"user_info\": {\"hobby\": \"Basketball\", \"name\": \"John\"}}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--e778b76a-ad47-4954-b792-8528883f753f-0', tool_calls=[{'name': 'save_user_info', 'args': {'user_info': {'hobby': 'Basketball', 'name': 'John'}}, 'id': '6f64da18-95b5-4ead-a245-d1166a046d22', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2499, 'output_tokens': 29, 'total_tokens': 2528, 'input_token_details': {'cache_read': 0}}),\n",
       "  ToolMessage(content='Successfully saved user info ', name='save_user_info', id='3ec5d188-680a-4826-8eeb-ee8f37635a63', tool_call_id='6f64da18-95b5-4ead-a245-d1166a046d22'),\n",
       "  AIMessage(content=\"That's great, John! I've noted that you like to play basketball.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--ca30353d-e974-45ae-bbe4-f37860192835-0', usage_metadata={'input_tokens': 2549, 'output_tokens': 18, 'total_tokens': 2567, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f51307c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item(namespace=['users'], key='user_23', value={'hobby': 'Basketball', 'name': 'John'}, created_at='2025-08-31T19:16:11.288340+00:00', updated_at='2025-08-31T19:16:11.288340+00:00')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get((\"users\",), \"user_23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df2d2d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'python_repl', 'arguments': '{\"code\": \"\\\\ndef is_prime(num):\\\\n    if num < 2:\\\\n        return False\\\\n    for i in range(2, int(num**0.5) + 1):\\\\n        if num % i == 0:\\\\n            return False\\\\n    return True\\\\n\\\\nprint(is_prime(7))\\\\nprint(is_prime(10))\\\\nprint(is_prime(2))\\\\nprint(is_prime(1))\\\\nprint(is_prime(23))\\\\nprint(is_prime(49))\\\\n\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--86dd4bf9-19f6-4037-a009-305e18e2f64c-0' tool_calls=[{'name': 'python_repl', 'args': {'code': '\\ndef is_prime(num):\\n    if num < 2:\\n        return False\\n    for i in range(2, int(num**0.5) + 1):\\n        if num % i == 0:\\n            return False\\n    return True\\n\\nprint(is_prime(7))\\nprint(is_prime(10))\\nprint(is_prime(2))\\nprint(is_prime(1))\\nprint(is_prime(23))\\nprint(is_prime(49))\\n'}, 'id': '5d7091fc-3e64-45c4-a80c-f59468f70d91', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2121, 'output_tokens': 190, 'total_tokens': 2311, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 55}}\n"
     ]
    }
   ],
   "source": [
    "print(chunk['messages'][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89815fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tools': {'messages': [ToolMessage(content='True\\nFalse\\nTrue\\nFalse\\nTrue\\nFalse\\n', name='python_repl', id='277e4232-a778-4516-9df3-5cbe33cccdd7', tool_call_id='5d7091fc-3e64-45c4-a80c-f59468f70d91')]}}\n",
      "\n",
      "\n",
      "{'planner_node': {'messages': AIMessage(content=['The following Python code defines a function `is_prime` that checks if a number is prime. It then tests the function with several examples:', '```python\\ndef is_prime(num):\\n    if num < 2:\\n        return False\\n    for i in range(2, int(num**0.5) + 1):\\n        if num % i == 0:\\n            return False\\n    return True\\n\\n# Test cases\\nprint(is_prime(7))   # Expected: True\\nprint(is_prime(10))  # Expected: False\\nprint(is_prime(2))   # Expected: True\\nprint(is_prime(1))   # Expected: False\\nprint(is_prime(23))  # Expected: True\\nprint(is_prime(49))  # Expected: False\\n```'], additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--8f9bc6eb-ed09-43db-8d0a-869d227d1e59-0', usage_metadata={'input_tokens': 2288, 'output_tokens': 184, 'total_tokens': 2472, 'input_token_details': {'cache_read': 0}})}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "user_input = input(\"Do you accept: ?\")\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    Command(resume=[{\"type\": user_input}]),\n",
    "    config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f4dc897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='tell me about pyramid in detail and save it in a txt file', additional_kwargs={}, response_metadata={}, id='3d461e2c-da90-4ac8-a295-a358610987ae'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'wikipedia', 'arguments': '{\"query\": \"pyramid\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--49e40eaf-89df-4ccd-adbb-30b829354499-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'pyramid'}, 'id': '613a6772-c5b7-4248-a375-c8da09528e03', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1696, 'output_tokens': 63, 'total_tokens': 1759, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 49}}), ToolMessage(content='Page: Pyramid\\nSummary: A pyramid (from Ancient Greek  πυραμίς (puramís) \\'pyramid\\', from the Egyptian pir-em-us, the vertical height of the structure.) is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape, such as triangular or quadrilateral, and its surface-lines either filled or stepped.\\n\\nA pyramid has the majority of its mass closer to the ground with less mass towards the pyramidion at the apex. This is due to the gradual decrease in the cross-sectional area along the vertical axis with increasing elevation. This offers a weight distribution that allowed early civilizations to create monumental structures.Ancient civilizations in many parts of the world pioneered the building of  pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt—the latter is the only extant example of the Seven Wonders of the Ancient World.\\n\\nPage: Great Pyramid of Giza\\nSummary: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c.\\u20092600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. It is the most famous monument of the Giza pyramid complex, which is part of the UNESCO World Heritage Site \"Memphis and its Necropolis\". It is situated at the northeastern end of the line of the three main pyramids at Giza.\\nInitially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. Over time, most of the smooth white limestone casing was removed, which lowered the pyramid\\'s height to the current 138.5 metres (454.4 ft); what is seen today is the underlying core structure. The base was measured to be about 230.3 metres (755.6 ft) square, giving a volume of roughly 2.6 million cubic metres (92 million cubic feet), which includes an internal hillock. The dimensions of the pyramid were 280 royal cubits (146.7 m; 481.4 ft) high, a base length of 440 cubits (230.6 m; 756.4 ft), with a seked of \\u20605+1/2\\u2060 palms (a slope of 51°50\\'40\").\\nThe Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. The majority of the stones are not uniform in size or shape, and are only roughly dressed. The outside layers were bound together by mortar. Primarily local limestone from the Giza Plateau was used for its construction. Other blocks were imported by boat on the Nile: white limestone from Tura for the casing, and blocks of granite from Aswan, weighing up to 80 tonnes, for the \"King\\'s Chamber\" structure.\\nThere are three known chambers inside of the Great Pyramid. The lowest was cut into the bedrock, upon which the pyramid was built, but remained unfinished. The so-called Queen\\'s Chamber and King\\'s Chamber, which contain a granite sarcophagus, are above ground, within the pyramid structure. Hemiunu, Khufu\\'s vizier, is believed by some to be the architect of the Great Pyramid. Many varying scientific and alternative hypotheses attempt to explain the exact construction techniques, but, as is the case for other such structures, there is no definite consensus.\\nThe funerary complex around the pyramid consisted of two mortuary temples connected by a causeway (one close to the pyramid and one near the Nile); tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \"satellite pyramid\"; and five buried solar barques.\\n\\nPage: Egyptian pyramids\\nSummary: The Egyptian pyramids are ancient masonry structures located in Egypt.', name='wikipedia', id='daa9944e-0c2a-45a5-a325-72b58530479a', tool_call_id='613a6772-c5b7-4248-a375-c8da09528e03'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'write_file', 'arguments': '{\"file_path\": \"pyramid_info.txt\", \"text\": \"Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids\\\\u2014first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\\\n\\\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \\\\\"satellite pyramid\\\\\"; and five buried solar barques.\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--4d4e2812-b668-4cc5-b2a1-f0bff508f82a-0', tool_calls=[{'name': 'write_file', 'args': {'file_path': 'pyramid_info.txt', 'text': 'Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\n\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \"satellite pyramid\"; and five buried solar barques.'}, 'id': 'ecd0eed6-7d18-4563-a1ba-730208ff2a72', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2658, 'output_tokens': 443, 'total_tokens': 3101, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 84}}), ToolMessage(content=\"Error: ValueError('Unsupported interrupt response type: cancel')\\n Please fix your mistakes.\", name='write_file', id='62f23f96-304c-4e7d-b237-45f422b0af94', tool_call_id='ecd0eed6-7d18-4563-a1ba-730208ff2a72', status='error'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'write_file', 'arguments': '{\"file_path\": \"pyramid_info.txt\", \"text\": \"Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids\\\\u2014first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\\\n\\\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \\\\\"satellite pyramid\\\\\"; and five buried solar barques.\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--362bb830-7024-4716-bfd5-68a2c176acce-0', tool_calls=[{'name': 'write_file', 'args': {'file_path': 'pyramid_info.txt', 'text': 'Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\n\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \"satellite pyramid\"; and five buried solar barques.'}, 'id': 'd092d5b0-3d7b-4080-976d-323f0c69e8bb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3051, 'output_tokens': 466, 'total_tokens': 3517, 'input_token_details': {'cache_read': 2684}, 'output_token_details': {'reasoning': 107}})]}, next=('tools',), config={'configurable': {'thread_id': '9550', 'checkpoint_ns': '', 'checkpoint_id': '1f08324c-e5d1-69b0-8005-e79030793f77'}}, metadata={'source': 'loop', 'step': 5, 'parents': {}}, created_at='2025-08-27T09:04:15.275460+00:00', parent_config={'configurable': {'thread_id': '9550', 'checkpoint_ns': '', 'checkpoint_id': '1f08324c-c650-6bb5-8004-b4fd6ac11b02'}}, tasks=(PregelTask(id='443e2efb-43ee-3ed3-8d0b-973e7ad4f601', name='tools', path=('__pregel_pull', 'tools'), error=None, interrupts=(Interrupt(value=[{'action_request': {'action': 'write_file', 'args': {'file_path': 'pyramid_info.txt', 'text': 'Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\n\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \"satellite pyramid\"; and five buried solar barques.'}}, 'config': {'allow_accept': True, 'allow_edit': True, 'allow_respond': True}, 'description': 'Please review the tool call'}], id='203e8b26e132ac7a67598b4a48c3da77'),), state=None, result=None),), interrupts=(Interrupt(value=[{'action_request': {'action': 'write_file', 'args': {'file_path': 'pyramid_info.txt', 'text': 'Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\n\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \"satellite pyramid\"; and five buried solar barques.'}}, 'config': {'allow_accept': True, 'allow_edit': True, 'allow_respond': True}, 'description': 'Please review the tool call'}], id='203e8b26e132ac7a67598b4a48c3da77'),))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f04ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=MvNdgmM7uyc\", add_video_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6812bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "ytt_api = YouTubeTranscriptApi()\n",
    "transcript = ytt_api.fetch(\"MvNdgmM7uyc\").to_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1d02b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi this is Lan from Lang chain I want totalk about using Lang graph for codeGeneration Um so co- generation is oneof the really interesting applicationsof llms like we've seen projects likeGitHub co-pilot become extremely popularum and a few weeks ago a paper came outum by the folks at codium AI calledAlpha codium and this was really coolpaper in particular because itintroduced this ideaof doing code generation using what youcan think of as like flow engineering soinstead of just like an llm a codingprompt like solve this problem and asolution what it does is it generates aset of solutions ranks them so that'sfine that's like kind of standard likekind of prompt response style flow butwhat it does here that I want to drawyour attention to is if it actuallytests that code in a few different wayson public tests and on AI generatedtests and the key point is this itactually iterates and tries to improvethe solution based upon those testresults so that was reallyinteresting and a tweet came out bykarpathy on this theme which kind ofmentions hey this idea of flowengineering is a really nice Paradigmmoving away from just like naive promptanswer toflow where you can build up an answeritely over time usingtesting so it's a really nice idea andwhat's kind of cool is a few weeks agowe introduced Lang graph as a way tobuild kind of arbitrary graphs which canrepresent different kinds of flows andI've done some videos on this previouslytalking about Lang graph or things likerag where like you can do retrieval andthen you can do like a retrieval qualitycheck like grade the documents ifthey're not good you can like try toretrieve again or you can like do a websearch or something but it's a way torepresent arbitrary kind of logicalflows withllms in a lot of the same way we do withagents but the benefit of graphs is thatyou can outline a flow that's a littlebit more it's kind of like an agent withguardrails it's like you define thesteps in a very particular order andevery time you run the graph it justexecutes in thatorder so what I want to do is I want totry to implement some of the ideas fromalpha codium using L graph and we'regoing to do that right now so inparticular let's say we want to answercoding questions about some part of theLang chain documentation and for thisI'm going to choose the L chainexpression language docs so it's asubset of our docs it's around 60,000tokens and it focuses only on line chainexpression language which is basically away you can represent chains usinginline chain and we'll talk about thatin a littlebit but I want to do a few simple thingsso I want to have one what we're goingto call a node in our graph that takes aquestion and outputs an answer usingLang chain expression language docs as areference and then with that answer Iwant to be able to parse out componentsso given the answer I want to be able toparse out like the Preamble what is thisanswering the import specifically andthen the code and to do this I want touse some like a pedantic object so it'slike very nicelyformatted if I have that I can reallyeasily Implement tests for things likecheck to make sure the Imports workcheck to make sure the code executes andif either of those fail I can loop backto my generation node and say Hey tryagain here's the error Trace so againwhat they're doing in Al codium is waymore sophisticated I don't mean tosuggest we're iing imple M this as is umthis actually works on like a bunch ofpublic coding challenges it actually hastests um for each question that are bothadd and publicly available so againwe're doing something much simpler but Iwant to show how you can Implement thesekinds of ideas and you can make itarbitrarily complex if youwant so I'm going to copy over some codeinto a notebook that I have running andall I've done is I've just done some pipinstalls and I've BAS to find a fewenvironment variables for Lang Smithwhich we'll see later is prettyuseful and I'm going to call thisdocs so this is where I'm going toingest the docs related to Langexpression language and I'm going tokick off uh this right now so that'srunning so again this is using a URLloader grab all the docs sort them andclean them a little bit and here we goso here we go these are all the docsrelated to Lang and expression languageit's around 60,000 token tokens I'vemeasured it in the past so there's ourdocs now I I want to show you somethingthat's very useful um I'll call it tooluse um with open ey models and and otherLMS have similar functionality but Iwant to show you something that's reallyuseful um what I'm going to do here isshow how to build a chain that willoutput remember we talked about in ourdiagram we want three things for everysolution we want a preamble we want wantImports we want code as a structuredobject that we can like work withindividually I'll show you right herehow to do that so we're doing is we'reimporting um uh from pantic this basemodel and field and we're defining adata model for our output so I want aprefix which is just like the plainlanguage solution like here's the setupto the problem the import statement andthe code I want those as three distinctthings that I can work withlater I'll use dpd4 uh1 25 say 128context window model um and what I'mgoing to do is I'm going to take thisthis data model I'm going to turn itinto a tool and I'm going to bind it tomy model and so basically What'sHappening Here is it's going to alwaysperform a function call to attempt toOutput in this format I specify herethat's all it's happening I Define aprompt that says here's all the L cellthey're Lang CH expression languagepronounced or or substituted as LChere's all LCL docs answer the questionsstructure your output in a few ways butwhat's cool is we're always growing thatfunction call to basically try to Outputa pantic object so there we go nowwhat's nice is I can just invoke thiswith a question so let's just trythat so I'm going to sayquestion and I'm going to say how tocreate a rag chain in NLC we want torunum okay this needs to be a dict there wego boom so that'srunning now this is actually just we cansee right here we passed in all thosedocs that we previously loaded so it'slike 60,000 tokens of context and againyou think about newer long context llmslike Gemini that becomes more morefeasible to do do things like this takelike a whole a whole code base a wholeset of documentation load it and juststuff it into a model and have it sayhey answer questions about this that'sstill running now the latency isdefinitely higher because it's very verylarge context but that's fine we have alittle bit of time and we can go over toLang Smith Al this is running and have alook so we can see here was our promptokay so there you go look at this 63,000tokens you can see it's a lot of contextum that's fine and we can actually seeit all here so it's in Langs Smith um wedon't want to scroll through all thatmess but you can see we've asked aquestion we're grounding the response inall this L cell docs and we're going tohopefully output the response as apedantic object that we can play with solet's just see and okay nice it's doneso you can see our object here has aprefix um and it actually has um it'salso going to have our Imports here aswell we can actually can see that inlsmith uh the answer is going to be hereand there you go see your Imports yourcode and your prefix and these can allbe extracted from that object uh reallyeasily um so it's basically a list andit's a pantic object code and you canextract each one just like Co you knowanswer. prefix answer do uh whatever ourvariables or whatever our keys areanswer. Imports answer. code so that'sgreat so that just shows you how tooluse Works um and how we can get thestructured output out of our generationnodenow what I'm going to do here is nowthat we've established that we can dothat I'm going to start setting up ourgraph and what I'm going to do is firstI'm going to find our graph state sothis is just going to be a dictionarywhich contains things relevant to ourproblem it'll contain our code solutionit'll contain any errors and that's allwe're going toneed and here is where this is all thecode related to My Graph and we're goingto walk through this so don't worry toomuch but I just want to kind of get thisall hereso here's our code now if we go up theway to think about this is simply thisum I want to go back to my diagram hereso every node in our graph just has acorresponding function and that functionmodifies the state in some way so what'shappening is our generation node isgoing to be working with question anditeration those are the parts of statethat we want as like inputs you can seeit kind of maps to here you havequestion and you have iteration justcounts how many times you've tried thiswe'll see why that's interestinglater um this is exactly what we sawbefore data model llm tooluse all the same stuff right templatenow here's where it'sinteresting if our state contains anerror this error key what that means iswe've fed back from some of our testsand we have an error that's already beengenerated so we're retrying here's whyinteresting if we'reretrying what we're going to do isappend our prompt just like we saw abovewe're going to add something to ourprompt that says hey you tried thisbefore here was your solution we savedthat as generation key um and in ourstates you can see it's right here codesolution generation here is your errorplease retry to answer this so it's kindof like inducing a reflection based onyour priorgeneration anderror and retry so that's a veryimportant point because basically givesus that feedback from if there's amistake and either the Imports or theexecutions we're feeding that back togeneration and generation is going toretry with that information present sothat's that's all that's happening thereum and we're basically adding that tothe prompt um and we're then invokingthe chain with that error and then we'regetting a new code solution so againthat's if errors in our state dick if itisn't then we're going to go ahead andgenerate our solution just like we didabove same thing soeasy um one little thing is every timewe return the the basically we're goingto rewrite that output to the statewe're going to increment our iterationsand say Here's how many times we'vetried to answer this question that'sreally it and you can see that's all wedo return the generation return thequestion return the number of iterationseasy now here's what's kind of nice wetalked about having these two importchecks the check for imports to checkfor execution let's our check importnode just going to be really simple wehave oursolution from the solution we can getthe Imports out just like we showedabove this code solution Imports is fromour pantic object um I'll move it overso you can kind of see so a panticobject has Imports we can get theImports and all we do is just attempt toexecute the Imports if it fails we alerthey import check failed and here's thekey point we're just going to create anew key uh error in our dict identifyingthat hey there's an error present umsomething failed here and you'll seewe're going to use that later now oneother little trick if there was a priorerror in our state we're just going topend it so we do want to kind ofmaintain that um if there's like anaccumulation of errors as we runmultiple iterations we want to keepaccumulating them so we don't likerevert and make the same mistake wealready did on a future iteration sowe're going to maintain our set ofErrors now if there's no error here thenwe're going to rewrite none so we'regoing to say we're good keep going uhand basically the same thing with codeexecution right in that case we're justextracting our code and our Imports wecreate a code block of imports Plus Codetry to execute it again if it failswrite our error and append all priorerrors if it doesn't return none that'sit that's all you really need to knownow here note that we're going to do twokind of gates so we want to know if dideither of those tests fail and again allwe need to do is we can uh grab ourerror and remember if there is no errorthen if error is none keep going so herewe're at the code execution likedecision point so do you want to go tocode execution or do you want to likerevert back and retry so you can seehere if there's no error when we get tothe this point um then because we'vedone our import check if there's noerror there keep going go to codeexecution we can see we return this nodewe want to go to um and if there is anerror we can say hey return to thegenerate node so really what thesefunctions do so these are conditionaledges what these do is they do some kindof conditional check based upon like ouroutput state so again if there's anerror or not if there's no error ittells you go to this node if there is anerror it tells you go back to generatenode that's it same deal with decidingto finish again if there's no error andnow here's the iteration thing for thesake of Simplicity what I say is give itthree tries I don't want it to runarbitrarily long uh if there's no erroror if you try three times just endthat's it uh otherwise go back togenerate so again same kind of thingdecide to finished based upon um yeahbased upon whether or not there's anerror in our code execution or notthat's really it that's all we're doingso we can go down we already grabbed allthis now here is where we actuallyDefine our what we call ourworkflow um and so this is actuallywhere we defined all our edges and nodesas these functions and here's just wherewe kind of stitch them all together umso it's actually pretty straightforwardit just follows exactly like the diagramwe showed above um we like we'rebasically adding all of our nodes andwe're building our graph following thediagram that we show so we can go backto the diagram so like you can kind offollow along right set your entry pointit's generate add an edge generate checkcode Imports now our conditional Edge umso if we're going to decide to checkcode execution that was our functionhere right hereso if um basically depending on theoutput here we can decide the next nodeto go to so um if the output of thefunction says check codeexecution we go to that node if theoutput says go to generate we go back togenerate so these are where you specifythe logic of the next node you want togo to and same here so that's all we docompile it done and just map to thisdiagram um kind of like one to one sothat's actually prettystraightforward and there's just onelittle thing we now need to do uh we aregoing to goahead and try aquestion so here's a question I've I'verun a bunch of these tests already thisis a question that seems kind of randombut it's like we actually built a NEvalve set and so it's a question that wewe've sound that there's some problemswith so I want to show you why this ispretty cool I'm passing it text Key Foodin my prompt I want to process it withsome function process text how do I dothis using uh line transpressionlanguage so it's a weird question butyou'll see why it's kind of fun in ashort in a little bit and what I'm goingto do is I'm just going to run my graphso what we can see because we print outwhat happens at every step we're can tokind of follow along and see what'shappeninghere um so it's going to generatesolution now we can see this may take alittle bit because it's the same kind oflong context generation that we sawpreviously so this is now running we cango to Lang Smith and we can actuallyjust check this Lang graph and we cansee okay so it's loading up and we're atgenerate so it's actually doing thisgeneration this is still pending here isall our input docs so you can see thatum that uh you know we passed this verylarge context to our LM uh so that'scool okay so here's this is interestingso what's happening is it's goingthrough some checks so um the codeimport check worked decide to check codeexecution a decision testing codeexecutionhere's an interesting one code blockcheckfailed um decision retry so it'sactually doing aregenerationso okay let's see it looks like it cameto an answer um let's actually go andlook at what happened in our Lang graphto kind of understand what happened sowhat happened if we look at theum let's look at when weattempted yeah exactly so here let meactually pull up the errorhereum here was ourresponse um and what I want to show youis the error that we appended to ourpromptumand we can actually make this a bitfaster we canscroll this is the Crux of what I wantto show you um okay here it is so what'scool is our initial attempt to solvethis problem introduced an error therewas an execution error it unsupportedOpera for types dict and string sobasically it did something wrong and wepassed that in the prompt to the llmwhen it performs this retry so the ourinitial solution was here and it had a acoding error as noted but here you cansee we provide that error and we sayplease try to reans this structure youknow like the same instructions beforehere was uh here was the the questionand we can see okay so this is actuallythe test of code execution which nowworks we can see previously when wetried this uh it fails and this was theerror that error was passed along in theprompt like we just saw the new the newtest indeed Works our final solution isfunctionalcode that's it so you can kind of getsome intuition for the fact that whenyou have this retry Lube you can recoverfrom errors using a little bit ofreflection that's really the big idea umand again you get your answer outhere um and so there's a bunch of keysand we don't necessarily I'll show youquickly uh keys and then we can justlook at the generationkey umcool and it's going to be a list solet's just break it out so there it isthere's our code object we can seeprefix okay so there's the prefiximport uh and let's try code and heylet's just convince ourselves thisactually works so we can just run exeImports that worksexexec the code and this should work it'sdoing something there it tells a jokegreat um so this is pretty cool itinitially when to try to answer thisquestion produce an error and it thenretry by passing that error back to thecontext just like we outlined in ourgraph and on the second try it gets itcorrect so that's nice it's a goodexample of how you can do this feedbackand and reflection stuff now we actuallyhave done quite a bit more work on thisso I built an eval set of 20 Questionsrelated to Lang and expression languageand evaluated them all uh using thisapproach relative to not using Langgraph and here's the results I want tokind of draw your attention to thisbecause it's a pretty interesting resultfor the import check without Lang graphversus with Lang graph it's about thesame so Imports weren't really a problembefore this like retry reflection stuffImports were okay on oural set of 20questions I should make a note weactually ran this uh this is showingstandard errors we ran this four timesand so I basically accumulate theresults I compute standard errors youcan see that there's there's some degreeof statistical reasonable inness tothese results um in any case importchecks were were kind of fine without itbut here's a big difference there's abig difference in our code execution uhper performance with and without landgraph so before land graph if you justtry like kind of single shot answergeneration a lot of the times this waslike a 55% success rate many of thecases we saw code execution fail butwith Lang graph with this kind of retryand reflection stuff we actually can seethat the the success rate goes up toaround I believe it was 80% so it'saround like a almost a 50% Improvementin performance um with and without usingL graph so that was actually reallyimpressive and and it just shows thepower of like a very simple idea umattempting code generation with thesekinds of like just very simple checksand reflection can significantly bump upyour performance and again the alphacodium paper shows this in like a verysophisticated context but what's cool isthis is like a very simple idea you canimp Implement yourself in not much timeum and we have this all available as anotebook you can run this on any pieceof code you want so just take whateverdocuments want here Plum them in and youcan run this and you can test this outfor yourself but I've been reallyimpressed I think it's pretty cool umand in general I think lra's a reallynice way to build these kind of likereflective or self-reflectiveapplications where you can build thesefeedback loops to you can do a check ifthe check fails try again with that Fewith that uh feedbackpresent um in the retry and I'll justshow you we have a Blog coming out I'mnot sure there's anything else in thatblog I haven't already showed youum yeah not nothing really to highlightthis was our results again um this ismaybe a little bit clearer to see um butagain pretty significant Improvement inperformance simple idea uh I definitelyencourage you to experiment with this umand of course all this code will beavailable for you so um uh you know feelfree to experiment and let us know howit goes thankyou\n"
     ]
    }
   ],
   "source": [
    "output = \"\"\n",
    "for t in transcript:\n",
    "    output += t['text']\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83653772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.config import get_store\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "store = InMemoryStore() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2a88a447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item(namespace=['istyle'], key='use3', value={'tone': 'casual', 'format': '', 'language': ''}, created_at='2025-08-31T15:59:33.261498+00:00', updated_at='2025-08-31T15:59:33.261498+00:00')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get((\"istyle\",), \"use3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespaces in store\n",
    "(\"users\", user_id)         # user profile\n",
    "(\"preferences\", user_id)   # preferences\n",
    "(\"knowledge\", user_id)     # facts\n",
    "(\"tasks\", user_id)         # goals\n",
    "(\"interactions\", user_id)  # style/history\n",
    "(\"domain\", user_id)        # domain-specific info\n",
    "(\"world\", user_id)         # external/tool context\n",
    "(\"sessions\", user_id)      # session summaries\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
