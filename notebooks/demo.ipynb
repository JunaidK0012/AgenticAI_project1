{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langchain_core.tools import tool\n",
    "from typing import TypedDict,Annotated\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "from langchain_core.messages import HumanMessage,BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage,ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import Command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146286a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.config import get_stream_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce21f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web search tool \n",
    "from langchain_tavily import TavilySearch\n",
    "web_search_tool = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File management tool\n",
    "from langchain_community.agent_toolkits import FileManagementToolkit\n",
    "\n",
    "working_directory = './'\n",
    "\n",
    "file_management_tools =FileManagementToolkit(\n",
    "    root_dir=str(working_directory),\n",
    "    selected_tools=[\"read_file\", \"write_file\", \"list_directory\"]\n",
    ").get_tools()\n",
    "\n",
    "\n",
    "read_tool, write_tool, list_tool = file_management_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b431f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool,tool\n",
    "from langgraph.prebuilt.interrupt import HumanInterruptConfig,HumanInterrupt\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.types import interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05944243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_human_in_the_loop(toolhitl,interrupt_config: HumanInterruptConfig = None) -> BaseTool:\n",
    "    \"\"\"Wrap a tool to support human-in-the-loop review.\"\"\"\n",
    "\n",
    "    if not isinstance(toolhitl, BaseTool):\n",
    "        toolhitl = tool(toolhitl)\n",
    "\n",
    "    if interrupt_config is None:\n",
    "        interrupt_config = {\n",
    "            \"allow_accept\":True,\n",
    "            \"allow_edit\": True,\n",
    "            \"allow_respond\":True\n",
    "        }\n",
    "\n",
    "    @tool(toolhitl.name,description=toolhitl.description,args_schema=toolhitl.args_schema)\n",
    "    def call_tool_with_interrupt(config: RunnableConfig, **tool_input):\n",
    "        request: HumanInterrupt = {\n",
    "            'action_request':{\n",
    "                \"action\":toolhitl.name,\n",
    "                \"args\":tool_input\n",
    "            },\n",
    "            \"config\":interrupt_config,\n",
    "            \"description\": \"Please review the tool call\"\n",
    "        }\n",
    "\n",
    "        response = interrupt([request])[0]\n",
    "\n",
    "        # approve the tool call\n",
    "        if response[\"type\"] == \"accept\":\n",
    "            tool_response = toolhitl.invoke(tool_input, config)\n",
    "        # update tool call args\n",
    "        elif response[\"type\"] == \"edit\":\n",
    "            tool_input = response[\"args\"][\"args\"]\n",
    "            tool_response = toolhitl.invoke(tool_input, config)\n",
    "        # respond to the LLM with user feedback\n",
    "        elif response[\"type\"] == \"response\":\n",
    "            user_feedback = response[\"args\"]\n",
    "            tool_response = user_feedback\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n",
    "\n",
    "        return tool_response\n",
    "    \n",
    "    return call_tool_with_interrupt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d45ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arxiv\n",
    "import arxiv\n",
    "\n",
    "@tool(\"arxiv_search\")\n",
    "def arxiv_search(query: str,max_results: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Searches arXiv for papers matching the query.\n",
    "    - query: keywords, authors or title\n",
    "    - max_results: number of papers to return\n",
    "    \"\"\"\n",
    "    try:\n",
    "        writer = get_stream_writer()\n",
    "        writer(f\"Looking up research papers for topic : {query}\")\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        papers = []\n",
    "        for result in search.results():\n",
    "            pdf_url = result.pdf_url if hasattr(result,\"pdf_url\") else result.entry_id.replace(\"abs\",\"pdf\")\n",
    "            papers.append(\n",
    "                f\"Title: {result.title}\\n\"\n",
    "                f\"Authors: {','.join(a.name for a in result.authors)}\\n\"\n",
    "                f'Published: {result.published.date()}\\n'\n",
    "                f\"Abstract: {result.summary.strip()}\\n\"\n",
    "                f\"Link: {result.entry_id}\\n\"\n",
    "                f\"PDF: {pdf_url}\\n\"\n",
    "                + \"-\"*80\n",
    "\n",
    "            )\n",
    "        if not papers:\n",
    "            return f\"No results found for '{query}\"\n",
    "        writer(f\"Acquired research papers for topic: {query}\")\n",
    "        return \"\\n\".join(papers)\n",
    "    except Exception as e:\n",
    "        return f\"Error during arXiv search: {e}\"\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0cb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikipedia\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(load_all_available_meta=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa930a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#youtube\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "youtube_tool = YouTubeSearchTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41788055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.tools import Tool,tool\n",
    "python_repl = PythonREPL()\n",
    "# You can create the tool to pass to an agent\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class PythonREPLInput(BaseModel):\n",
    "    code: str\n",
    "\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Input should be Python code as a string.\",\n",
    "    args_schema=PythonREPLInput,\n",
    "    func=lambda code: python_repl.run(code)  # map `code` -> REPL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1daf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "#from langgraph.store.sqlite import SqliteStore\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.config import get_store\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Optional\n",
    "\n",
    "store = InMemoryStore() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ae933",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_user_info(config: RunnableConfig) -> str:\n",
    "    \"\"\"Look up user info.\"\"\"\n",
    "    store = get_store()\n",
    "    user_id = config['configurable'].get(\"user_id\")\n",
    "    user_info = store.get((\"users\",),user_id)\n",
    "    return str(user_info.value) if user_info else \"Unknown user\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "    \n",
    "\n",
    "@tool \n",
    "def save_user_info(user_info: Dict[str, Any], config: RunnableConfig) -> str:\n",
    "    \"\"\"\n",
    "    Save arbitrary user info as key-value pairs.\n",
    "    Always pass `user_info` as a JSON object (not a string).\n",
    "    Example: {\"name\": \"John\", \"age\": 30}\n",
    "    \"\"\"\n",
    "    store = get_store()\n",
    "    user_id = config['configurable'].get(\"user_id\")\n",
    "    store.put((\"users\",), user_id, user_info)\n",
    "    return \"Successfully saved user info \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "tools = [get_user_info,save_user_info,add_human_in_the_loop(repl_tool),arxiv_search,wikipedia_tool,youtube_tool,read_tool, add_human_in_the_loop(write_tool), list_tool,web_search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tools:\n",
    "    print(t.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm\n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-2.5-flash')\n",
    "llm_with_tools = llm.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93797cc",
   "metadata": {},
   "source": [
    "GRAPH BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmem.short_term import SummarizationNode, RunningSummary\n",
    "from langchain_core.messages import AnyMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52228cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages : Annotated[list[BaseMessage],add_messages]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
    "from langchain_core.messages import BaseMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e79e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "CURRENT_TIME_IST = datetime.now().astimezone().strftime(\"%Y-%m-%d %H:%M %Z\")\n",
    "\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an intelligent reasoning agent that helps users by combining natural conversation \n",
    "with external tools when needed.\n",
    "\n",
    "tools available :  arxiv_search, read_tool, write_tool, list_tool, duck_search, tavily_search, wikipedia_tool,\n",
    "    youtube_search_tool, youtube_transcript_tool, repl_tool, add_event, list_events, read_webpage,\n",
    "    generate_pdf, shopping_search, create_ticket, list_tickets, get_ticket_details,  update_ticket, news_search,\n",
    "\n",
    "\n",
    "Current date/time: {CURRENT_TIME_IST}\n",
    "\n",
    "\n",
    "\n",
    "### Reasoning Framework\n",
    "Follow the ReAct reasoning loop:\n",
    "1. **Thought** — explain what you are thinking or planning.\n",
    "2. **Action** — choose the correct tool to use.\n",
    "3. **Action Input** — provide the exact structured input for the tool.\n",
    "4. **Observation** — read the tool's result and update your reasoning.\n",
    "\n",
    "Repeat this loop until you can confidently respond to the user.\n",
    "\n",
    "### Style & Tone\n",
    "- Be concise but complete.\n",
    "- Use plain language that non-technical users can understand.\n",
    "- If user input is ambiguous, ask clarifying questions before acting.\n",
    "- Never hallucinate tool outputs. If unsure, say so.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner_node(state: State):\n",
    "\n",
    "\n",
    "    planner_prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system',system_prompt),\n",
    "        MessagesPlaceholder(variable_name='messages')\n",
    "    ])\n",
    "\n",
    "\n",
    "    planner = planner_prompt | llm_with_tools\n",
    "    result = planner.invoke({'messages': state[\"messages\"]})\n",
    "\n",
    "\n",
    "    return ({'messages':result}) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)\n",
    "checkpointer = InMemorySaver()\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "graph.add_node('planner_node',planner_node)\n",
    "graph.add_node('tools',tool_node)\n",
    "\n",
    "graph.add_edge(START,'planner_node')\n",
    "\n",
    "graph.add_conditional_edges('planner_node',tools_condition)\n",
    "graph.add_edge('tools','planner_node')\n",
    "graph = graph.compile(checkpointer=checkpointer,store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe89fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"write a 4 line poem on bengal tiger save first 2 line in tr.txt save the last 2 line in er.txt\")]\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"fqo\" ,\"user_id\": \"user_00\"}}\n",
    "# Run the agent\n",
    "chunk =  graph.invoke(\n",
    "    initial_state,\n",
    "    config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "user_input = input(\"Do you accept: ?\")\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    Command(resume=[{\"type\": user_input}]),\n",
    "    config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
