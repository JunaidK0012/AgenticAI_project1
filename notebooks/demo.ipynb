{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2332a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langchain_core.tools import tool\n",
    "from typing import TypedDict,Annotated\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "from langchain_core.messages import HumanMessage,BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage,ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import Command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf7736f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "146286a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.config import get_stream_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce21f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web search tool \n",
    "from langchain_tavily import TavilySearch\n",
    "web_search_tool = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9628b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File management tool\n",
    "from langchain_community.agent_toolkits import FileManagementToolkit\n",
    "\n",
    "working_directory = './files'\n",
    "\n",
    "file_management_tools =FileManagementToolkit(\n",
    "    root_dir=str(working_directory),\n",
    "    selected_tools=[\"read_file\", \"write_file\", \"list_directory\"]\n",
    ").get_tools()\n",
    "\n",
    "\n",
    "read_tool, write_tool, list_tool = file_management_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b431f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool,tool\n",
    "from langgraph.prebuilt.interrupt import HumanInterruptConfig,HumanInterrupt\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.types import interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05944243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_human_in_the_loop(toolhitl,interrupt_config: HumanInterruptConfig = None) -> BaseTool:\n",
    "    \"\"\"Wrap a tool to support human-in-the-loop review.\"\"\"\n",
    "\n",
    "    if not isinstance(toolhitl, BaseTool):\n",
    "        toolhitl = tool(toolhitl)\n",
    "\n",
    "    if interrupt_config is None:\n",
    "        interrupt_config = {\n",
    "            \"allow_accept\":True,\n",
    "            \"allow_edit\": True,\n",
    "            \"allow_respond\":True\n",
    "        }\n",
    "\n",
    "    @tool(toolhitl.name,description=toolhitl.description,args_schema=toolhitl.args_schema)\n",
    "    def call_tool_with_interrupt(config: RunnableConfig, **tool_input):\n",
    "        request: HumanInterrupt = {\n",
    "            'action_request':{\n",
    "                \"action\":toolhitl.name,\n",
    "                \"args\":tool_input\n",
    "            },\n",
    "            \"config\":interrupt_config,\n",
    "            \"description\": \"Please review the tool call\"\n",
    "        }\n",
    "\n",
    "        response = interrupt([request])[0]\n",
    "\n",
    "        # approve the tool call\n",
    "        if response[\"type\"] == \"accept\":\n",
    "            tool_response = toolhitl.invoke(tool_input, config)\n",
    "        # update tool call args\n",
    "        elif response[\"type\"] == \"edit\":\n",
    "            tool_input = response[\"args\"][\"args\"]\n",
    "            tool_response = toolhitl.invoke(tool_input, config)\n",
    "        # respond to the LLM with user feedback\n",
    "        elif response[\"type\"] == \"response\":\n",
    "            user_feedback = response[\"args\"]\n",
    "            tool_response = user_feedback\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n",
    "\n",
    "        return tool_response\n",
    "    \n",
    "    return call_tool_with_interrupt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d45ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arxiv\n",
    "import arxiv\n",
    "\n",
    "@tool(\"arxiv_search\")\n",
    "def arxiv_search(query: str,max_results: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Searches arXiv for papers matching the query.\n",
    "    - query: keywords, authors or title\n",
    "    - max_results: number of papers to return\n",
    "    \"\"\"\n",
    "    try:\n",
    "        writer = get_stream_writer()\n",
    "        writer(f\"Looking up research papers for topic : {query}\")\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        papers = []\n",
    "        for result in search.results():\n",
    "            pdf_url = result.pdf_url if hasattr(result,\"pdf_url\") else result.entry_id.replace(\"abs\",\"pdf\")\n",
    "            papers.append(\n",
    "                f\"Title: {result.title}\\n\"\n",
    "                f\"Authors: {','.join(a.name for a in result.authors)}\\n\"\n",
    "                f'Published: {result.published.date()}\\n'\n",
    "                f\"Abstract: {result.summary.strip()}\\n\"\n",
    "                f\"Link: {result.entry_id}\\n\"\n",
    "                f\"PDF: {pdf_url}\\n\"\n",
    "                + \"-\"*80\n",
    "\n",
    "            )\n",
    "        if not papers:\n",
    "            return f\"No results found for '{query}\"\n",
    "        writer(f\"Acquired research papers for topic: {query}\")\n",
    "        return \"\\n\".join(papers)\n",
    "    except Exception as e:\n",
    "        return f\"Error during arXiv search: {e}\"\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bee6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JunaidKhan\\AppData\\Local\\Temp\\ipykernel_17264\\2627632007.py:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = arxiv_search(\"Quantum Machine Learning\")\n"
     ]
    }
   ],
   "source": [
    "result = arxiv_search(\"Quantum Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0cb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikipedia\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(load_all_available_meta=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa930a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#youtube\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "youtube_tool = YouTubeSearchTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b957f552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['https://www.youtube.com/shorts/y9XnfQdOMRQ', 'https://www.youtube.com/shorts/dMBVuRYzAKs']\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_tool.run(\"CampusX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41788055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.tools import Tool,tool\n",
    "python_repl = PythonREPL()\n",
    "# You can create the tool to pass to an agent\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class PythonREPLInput(BaseModel):\n",
    "    code: str\n",
    "\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Input should be Python code as a string.\",\n",
    "    args_schema=PythonREPLInput,\n",
    "    func=lambda code: python_repl.run(code)  # map `code` -> REPL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1daf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "#from langgraph.store.sqlite import SqliteStore\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.config import get_store\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Optional\n",
    "\n",
    "store = InMemoryStore() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325ae933",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_user_info(config: RunnableConfig) -> str:\n",
    "    \"\"\"Look up user info.\"\"\"\n",
    "    store = get_store()\n",
    "    user_id = config['configurable'].get(\"user_id\")\n",
    "    user_info = store.get((\"users\",),user_id)\n",
    "    return str(user_info.value) if user_info else \"Unknown user\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b6f36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "    \n",
    "\n",
    "@tool \n",
    "def save_user_info(user_info: Dict[str, Any], config: RunnableConfig) -> str:\n",
    "    \"\"\"\n",
    "    Save arbitrary user info as key-value pairs.\n",
    "    Always pass `user_info` as a JSON object (not a string).\n",
    "    Example: {\"name\": \"John\", \"age\": 30}\n",
    "    \"\"\"\n",
    "    store = get_store()\n",
    "    user_id = config['configurable'].get(\"user_id\")\n",
    "    store.put((\"users\",), user_id, user_info)\n",
    "    return \"Successfully saved user info \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "243a1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "tools = [add_human_in_the_loop(repl_tool),arxiv_search,wikipedia_tool,youtube_tool,read_tool, add_human_in_the_loop(write_tool), list_tool,web_search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a39f62f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Python shell. Input should be Python code as a string.\n",
      "Searches arXiv for papers matching the query.\n",
      "- query: keywords, authors or title\n",
      "- max_results: number of papers to return\n",
      "A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "search for youtube videos associated with a person. the input to this tool should be a comma separated list, the first part contains a person name and the second a number that is the maximum number of video results to return aka num_results. the second part is optional\n",
      "Read file from disk\n",
      "Write file to disk\n",
      "List files and directories in a specified folder\n",
      "A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. It not only retrieves URLs and snippets, but offers advanced search depths, domain management, time range filters, and image search, this tool delivers real-time, accurate, and citation-backed results.Input should be a search query.\n"
     ]
    }
   ],
   "source": [
    "for t in tools:\n",
    "    print(t.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be4a34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm\n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-2.5-flash')\n",
    "llm_with_tools = llm.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93797cc",
   "metadata": {},
   "source": [
    "GRAPH BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52228cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages : Annotated[list[BaseMessage],add_messages]\n",
    "    llm_input_messages: list[BaseMessage]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d617a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
    "from langchain_core.messages import BaseMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4584a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_model_hook(state: State):\n",
    "    \"\"\"\n",
    "    This function will be called prior to every llm call to prepare the message for the llm.\n",
    "    \"\"\"\n",
    "\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=3000,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    return {\"llm_input_messages\": trimmed_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79288f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17e79e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "CURRENT_TIME_IST = datetime.now().astimezone().strftime(\"%Y-%m-%d %H:%M %Z\")\n",
    "\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an intelligent reasoning agent that helps users by combining natural conversation \n",
    "with external tools when needed.\n",
    "\n",
    "tools available :  arxiv_search, read_tool, write_tool, list_tool, duck_search, tavily_search, wikipedia_tool,\n",
    "    youtube_search_tool, youtube_transcript_tool, repl_tool, add_event, list_events, read_webpage,\n",
    "    generate_pdf, shopping_search, create_ticket, list_tickets, get_ticket_details,  update_ticket, news_search,\n",
    "\n",
    "\n",
    "Current date/time: {CURRENT_TIME_IST}\n",
    "\n",
    "\n",
    "\n",
    "### Reasoning Framework\n",
    "Follow the ReAct reasoning loop:\n",
    "1. **Thought** — explain what you are thinking or planning.\n",
    "2. **Action** — choose the correct tool to use.\n",
    "3. **Action Input** — provide the exact structured input for the tool.\n",
    "4. **Observation** — read the tool's result and update your reasoning.\n",
    "\n",
    "Repeat this loop until you can confidently respond to the user.\n",
    "\n",
    "### Style & Tone\n",
    "- Be concise but complete.\n",
    "- Use plain language that non-technical users can understand.\n",
    "- If user input is ambiguous, ask clarifying questions before acting.\n",
    "- Never hallucinate tool outputs. If unsure, say so.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa66dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner_node(state):\n",
    "    user_query = state[\"llm_input_messages\"]\n",
    "\n",
    "    planner_prompt = ChatPromptTemplate([\n",
    "        ('system',system_prompt),\n",
    "        MessagesPlaceholder(variable_name='llm_input_messages')\n",
    "    ])\n",
    "\n",
    "    planner = planner_prompt | llm_with_tools\n",
    "    result = planner.invoke({'llm_input_messages': state['llm_input_messages']})\n",
    "\n",
    "\n",
    "    return ({'messages':result}) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "772b7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(State)\n",
    "checkpointer = InMemorySaver()\n",
    "tool_node = ToolNode(tools)\n",
    "graph.add_node(\"pre_model_hook\", pre_model_hook)\n",
    "graph.add_node('planner_node',planner_node)\n",
    "graph.add_node('tools',tool_node)\n",
    "\n",
    "graph.add_edge(START,'pre_model_hook')\n",
    "graph.add_edge('pre_model_hook','planner_node')\n",
    "graph.add_conditional_edges('planner_node',tools_condition)\n",
    "graph.add_edge('tools','pre_model_hook')\n",
    "graph = graph.compile(checkpointer=checkpointer,store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abe89fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d733a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"indias matches schedule for asia cup 2025\")]\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"cvsjyyq0\" ,\"user_id\": \"user_300\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream_mode, chunk in graph.stream(\n",
    "    initial_state,\n",
    "    stream_mode=[\"updates\", \"messages\"],\n",
    "    config=config\n",
    "):\n",
    "    print(f\"\\n=== MODE: {stream_mode} ===\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238142b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AIMessageChunk(content='', additional_kwargs={'function_call': {'name': 'tavily_search', 'arguments': '{\"query\": \"India\\'s match schedule Asia Cup 2025\"}'}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--a65c1279-e55a-42a2-b881-bbfc9b1c0713', tool_calls=[{'name': 'tavily_search', 'args': {'query': \"India's match schedule Asia Cup 2025\"}, 'id': 'eb239922-aeba-4c12-acc7-e4ac6188ee8e', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1998, 'output_tokens': 86, 'total_tokens': 2084, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 58}}, tool_call_chunks=[{'name': 'tavily_search', 'args': '{\"query\": \"India\\'s match schedule Asia Cup 2025\"}', 'id': 'eb239922-aeba-4c12-acc7-e4ac6188ee8e', 'index': None, 'type': 'tool_call_chunk'}]), {'thread_id': 'cvsjyyq0', 'user_id': 'user_300', 'langgraph_step': 2, 'langgraph_node': 'planner_node', 'langgraph_triggers': ('branch:to:planner_node',), 'langgraph_path': ('__pregel_pull', 'planner_node'), 'langgraph_checkpoint_ns': 'planner_node:fe5c79d6-a608-28c0-d158-9ac8cdb5841c', 'checkpoint_ns': 'planner_node:fe5c79d6-a608-28c0-d158-9ac8cdb5841c', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7})\n",
      "<class 'tuple'>\n",
      "(ToolMessage(content='{\"query\": \"India\\'s match schedule Asia Cup 2025\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://timesofindia.indiatimes.com/sports/cricket/asia-cup/india-asia-cup-2025-schedule-full-fixtures-match-dates-venues-ind-vs-pak-clash-timings-and-more/articleshow/123760520.cms\", \"title\": \"India Asia Cup 2025 Schedule: Full Fixtures, Match Dates ...\", \"content\": \"NEW DELHI: Reigning champions India, under the leadership of Suryakumar Yadav, are set to launch their Asia Cup 2025 journey on September 10 against tournament hosts United Arab Emirates at the Dubai International Cricket Stadium. | Sep 10 (Wed) | India vs United Arab Emirates, 2nd Match, Group A | Dubai International Cricket Stadium, Dubai | 8:00 PM | 10:30 AM | 3:30 PM | | Sep 14 (Sun) | India vs Pakistan, 6th Match, Group A | Dubai International Cricket Stadium, Dubai | 8:00 PM | 10:30 AM | 3:30 PM | | Sep 19 (Fri) | India vs Oman, 12th Match, Group A | Sheikh Zayed Stadium, Abu Dhabi | 8:00 PM | 10:30 AM | 3:30 PM | * IND vs UAE: Gautam Gambhir wins hearts, signs autographs ahead of India\\'s Asia Cup opener\", \"score\": 0.9470345, \"raw_content\": null}, {\"url\": \"https://asiacupschedule.com/\", \"title\": \"Asia Cup 2025 Schedule: Groups, Fixtures, Teams & Venues\", \"content\": \"# Asia Cup 2025 Schedule: Groups, Fixtures, Teams & Venues | Qualified Teams | India, Pakistan, Afghanistan, Bangladesh, Sri Lanka, UAE, Hong Kong, Oman | Also Check: India’s schedule for the Asia Cup 2025 ## **Asia Cup 2025 Groups** The Asia Cup 2025 schedule is based on two groups, A and B. The final match will be played on **28 September 2025,** according to the Asia Cup 2025 dates. Eight teams have been participating in the Asia Cup 2025. India, Pakistan, Sri Lanka, Bangladesh, Afghanistan, UAE, Oman, and Hong Kong have qualified for the Asia Cup 2025. Eight teams will play 19 matches in the Asia Cup 2025. There are 4 teams in each group in the Asia Cup 2025.\", \"score\": 0.88799405, \"raw_content\": null}, {\"url\": \"https://indianexpress.com/section/sports/cricket/asia-cup-series/schedules-fixtures/\", \"title\": \"Asia Cup 2025 Schedule & Fixtures\", \"content\": \"# Asia Cup 2025 Schedule & Fixtures Match 1, September 09, 2025 at 20:00 IST. Zayed Cricket Stadium, Abu Dhabi Match 2, September 10, 2025 at 20:00 IST. Match 3, September 11, 2025 at 20:00 IST. Zayed Cricket Stadium, Abu Dhabi Match 4, September 12, 2025 at 20:00 IST. Match 5, September 13, 2025 at 20:00 IST. Zayed Cricket Stadium, Abu Dhabi Match 6, September 14, 2025 at 20:00 IST. Match 7, September 15, 2025 at 17:30 IST. Zayed Cricket Stadium, Abu Dhabi Zayed Cricket Stadium, Abu Dhabi Zayed Cricket Stadium, Abu Dhabi Zayed Cricket Stadium, Abu Dhabi Zayed Cricket Stadium, Abu Dhabi + My Express * My Express * Express Shorts * My Express * My Express\", \"score\": 0.8849276, \"raw_content\": null}], \"response_time\": 1.1, \"request_id\": \"e4998078-827a-4c62-a6b1-1862a8123b01\"}', name='tavily_search', id='9fe7af1e-d295-4774-9c91-cde5c25c3131', tool_call_id='eb239922-aeba-4c12-acc7-e4ac6188ee8e'), {'thread_id': 'cvsjyyq0', 'user_id': 'user_300', 'langgraph_step': 3, 'langgraph_node': 'tools', 'langgraph_triggers': ('branch:to:tools',), 'langgraph_path': ('__pregel_pull', 'tools'), 'langgraph_checkpoint_ns': 'tools:ce29e799-3613-a641-1c2e-fb4add51ea81', 'revision_id': '647430f-dirty'})\n",
      "<class 'tuple'>\n",
      "(AIMessageChunk(content=\"Here is India's match schedule for the Asia Cup 2025:\\n\\n*   **September 10 (Wednesday):** India vs United Arab Emirates, 2nd Match, Group A at Dubai International Cricket Stadium, Dubai (8\", additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--8488a775-8964-4a5a-8a25-a67c857aba66', usage_metadata={'input_tokens': 3044, 'output_tokens': 102, 'total_tokens': 3146, 'input_token_details': {'cache_read': 1796}, 'output_token_details': {'reasoning': 51}}), {'thread_id': 'cvsjyyq0', 'user_id': 'user_300', 'langgraph_step': 5, 'langgraph_node': 'planner_node', 'langgraph_triggers': ('branch:to:planner_node',), 'langgraph_path': ('__pregel_pull', 'planner_node'), 'langgraph_checkpoint_ns': 'planner_node:cd8b3178-a23f-c177-7a1c-0b736dbb7ada', 'checkpoint_ns': 'planner_node:cd8b3178-a23f-c177-7a1c-0b736dbb7ada', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7})\n",
      "<class 'tuple'>\n",
      "(AIMessageChunk(content=':00 PM IST)\\n*   **September 14 (Sunday):** India vs Pakistan, 6th Match, Group A at Dubai International Cricket Stadium, Dubai (8:00 PM IST)\\n*   **', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--8488a775-8964-4a5a-8a25-a67c857aba66', usage_metadata={'output_token_details': {'reasoning': 0}, 'total_tokens': 48, 'output_tokens': 48, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}}), {'thread_id': 'cvsjyyq0', 'user_id': 'user_300', 'langgraph_step': 5, 'langgraph_node': 'planner_node', 'langgraph_triggers': ('branch:to:planner_node',), 'langgraph_path': ('__pregel_pull', 'planner_node'), 'langgraph_checkpoint_ns': 'planner_node:cd8b3178-a23f-c177-7a1c-0b736dbb7ada', 'checkpoint_ns': 'planner_node:cd8b3178-a23f-c177-7a1c-0b736dbb7ada', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7})\n",
      "<class 'tuple'>\n",
      "(AIMessageChunk(content='September 19 (Friday):** India vs Oman, 12th Match, Group A at Sheikh Zayed Stadium, Abu Dhabi (8:00 PM IST)', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--8488a775-8964-4a5a-8a25-a67c857aba66', usage_metadata={'output_token_details': {'reasoning': 0}, 'total_tokens': 35, 'output_tokens': 35, 'input_tokens': 0, 'input_token_details': {'cache_read': 0}}), {'thread_id': 'cvsjyyq0', 'user_id': 'user_300', 'langgraph_step': 5, 'langgraph_node': 'planner_node', 'langgraph_triggers': ('branch:to:planner_node',), 'langgraph_path': ('__pregel_pull', 'planner_node'), 'langgraph_checkpoint_ns': 'planner_node:cd8b3178-a23f-c177-7a1c-0b736dbb7ada', 'checkpoint_ns': 'planner_node:cd8b3178-a23f-c177-7a1c-0b736dbb7ada', 'ls_provider': 'google_genai', 'ls_model_name': 'gemini-2.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7})\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessageChunk,ToolMessageChunk, HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "for stream_mode, chunk in graph.stream(\n",
    "    initial_state,\n",
    "    stream_mode=[\"updates\", \"messages\"],\n",
    "    config=config\n",
    "):\n",
    "    if stream_mode == \"messages\":\n",
    "        print(chunk)\n",
    "        print(type(chunk))\n",
    "        message, metadata = chunk \n",
    "        if isinstance(message, AIMessageChunk):\n",
    "            print(\"AIMessage\")\n",
    "            print(message.content)\n",
    "        elif isinstance(message, ToolMessage):\n",
    "            print(\"Tools\")\n",
    "            print(f\"🔧 **Using tool:** `{message.name}`\")\n",
    "\n",
    "    elif stream_mode == \"updates\":\n",
    "        if \"__interrupt__\" in chunk:\n",
    "            print(\"\\n\\n[INTERRUPT CAUGHT]\")\n",
    "            print(chunk[\"__interrupt__\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "feb1c797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream_mode: updates\n",
      "content: {'pre_model_hook': {'llm_input_messages': [HumanMessage(content='latest research on multimodal AI', additional_kwargs={}, response_metadata={}, id='4610db30-f960-47e2-8556-5c2aa3e087c5'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'arxiv_search', 'arguments': '{\"query\": \"multimodal AI\", \"max_results\": 5.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--88b4968e-b98b-4295-9cba-8dfd99540b28-0', tool_calls=[{'name': 'arxiv_search', 'args': {'query': 'multimodal AI', 'max_results': 5.0}, 'id': '46d7af1f-d1d4-49b5-afb1-54ea0d9fd75c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1991, 'output_tokens': 83, 'total_tokens': 2074, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 59}}), ToolMessage(content=\"Title: Toward AI-driven Multimodal Interfaces for Industrial CAD Modeling\\nAuthors: Jiin Choi,Yugyeong Jang,Kyung Hoon Hyun\\nPublished: 2025-03-21\\nAbstract: AI-driven multimodal interfaces have the potential to revolutionize\\nindustrial 3D CAD modeling by improving workflow efficiency and user\\nexperience. However, the integration of these technologies remains challenging\\ndue to software constraints, user adoption barriers, and limitations in AI\\nmodel adaptability. This paper explores the role of multimodal AI in CAD\\nenvironments, examining its current applications, key challenges, and future\\nresearch directions. We analyze Bayesian workflow inference, multimodal input\\nstrategies, and collaborative AI-driven interfaces to identify areas where AI\\ncan enhance CAD design processes while addressing usability concerns in\\nindustrial manufacturing settings.\\nLink: http://arxiv.org/abs/2503.16824v1\\nPDF: http://arxiv.org/pdf/2503.16824v1\\n--------------------------------------------------------------------------------\\nTitle: Navigating the landscape of multimodal AI in medicine: a scoping review on technical challenges and clinical applications\\nAuthors: Daan Schouten,Giulia Nicoletti,Bas Dille,Catherine Chia,Pierpaolo Vendittelli,Megan Schuurmans,Geert Litjens,Nadieh Khalili\\nPublished: 2024-11-06\\nAbstract: Recent technological advances in healthcare have led to unprecedented growth\\nin patient data quantity and diversity. While artificial intelligence (AI)\\nmodels have shown promising results in analyzing individual data modalities,\\nthere is increasing recognition that models integrating multiple complementary\\ndata sources, so-called multimodal AI, could enhance clinical decision-making.\\nThis scoping review examines the landscape of deep learning-based multimodal AI\\napplications across the medical domain, analyzing 432 papers published between\\n2018 and 2024. We provide an extensive overview of multimodal AI development\\nacross different medical disciplines, examining various architectural\\napproaches, fusion strategies, and common application areas. Our analysis\\nreveals that multimodal AI models consistently outperform their unimodal\\ncounterparts, with an average improvement of 6.2 percentage points in AUC.\\nHowever, several challenges persist, including cross-departmental coordination,\\nheterogeneous data characteristics, and incomplete datasets. We critically\\nassess the technical and practical challenges in developing multimodal AI\\nsystems and discuss potential strategies for their clinical implementation,\\nincluding a brief overview of commercially available multimodal AI models for\\nclinical decision-making. Additionally, we identify key factors driving\\nmultimodal AI development and propose recommendations to accelerate the field's\\nmaturation. This review provides researchers and clinicians with a thorough\\nunderstanding of the current state, challenges, and future directions of\\nmultimodal AI in medicine.\\nLink: http://arxiv.org/abs/2411.03782v1\\nPDF: http://arxiv.org/pdf/2411.03782v1\\n--------------------------------------------------------------------------------\\nTitle: Multimodal foundation models are better simulators of the human brain\\nAuthors: Haoyu Lu,Qiongyi Zhou,Nanyi Fei,Zhiwu Lu,Mingyu Ding,Jingyuan Wen,Changde Du,Xin Zhao,Hao Sun,Huiguang He,Ji-Rong Wen\\nPublished: 2022-08-17\\nAbstract: Multimodal learning, especially large-scale multimodal pre-training, has\\ndeveloped rapidly over the past few years and led to the greatest advances in\\nartificial intelligence (AI). Despite its effectiveness, understanding the\\nunderlying mechanism of multimodal pre-training models still remains a grand\\nchallenge. Revealing the explainability of such models is likely to enable\\nbreakthroughs of novel learning paradigms in the AI field. To this end, given\\nthe multimodal nature of the human brain, we propose to explore the\\nexplainability of multimodal learning models with the aid of non-invasive brain\\nimaging technologies such as functional magnetic resonance imaging (fMRI).\\nConcretely, we first present a newly-designed multimodal foundation model\\npre-trained on 15 million image-text pairs, which has shown strong multimodal\\nunderstanding and generalization abilities in a variety of cognitive downstream\\ntasks. Further, from the perspective of neural encoding (based on our\\nfoundation model), we find that both visual and lingual encoders trained\\nmultimodally are more brain-like compared with unimodal ones. Particularly, we\\nidentify a number of brain regions where multimodally-trained encoders\\ndemonstrate better neural encoding performance. This is consistent with the\\nfindings in existing studies on exploring brain multi-sensory integration.\\nTherefore, we believe that multimodal foundation models are more suitable tools\\nfor neuroscientists to study the multimodal signal processing mechanisms in the\\nhuman brain. Our findings also demonstrate the potential of multimodal\\nfoundation models as ideal computational simulators to promote both\\nAI-for-brain and brain-for-AI research.\\nLink: http://arxiv.org/abs/2208.08263v1\\nPDF: http://arxiv.org/pdf/2208.08263v1\\n--------------------------------------------------------------------------------\\nTitle: Multimodal Conversational AI: A Survey of Datasets and Approaches\\nAuthors: Anirudh Sundar,Larry Heck\\nPublished: 2022-05-13\\nAbstract: As humans, we experience the world with all our senses or modalities (sound,\\nsight, touch, smell, and taste). We use these modalities, particularly sight\\nand touch, to convey and interpret specific meanings. Multimodal expressions\\nare central to conversations; a rich set of modalities amplify and often\\ncompensate for each other. A multimodal conversational AI system answers\\nquestions, fulfills tasks, and emulates human conversations by understanding\\nand expressing itself via multiple modalities. This paper motivates, defines,\\nand mathematically formulates the multimodal conversational research objective.\\nWe provide a taxonomy of research required to solve the objective: multimodal\\nrepresentation, fusion, alignment, translation, and co-learning. We survey\\nstate-of-the-art datasets and approaches for each research area and highlight\\ntheir limiting assumptions. Finally, we identify multimodal co-learning as a\\npromising direction for multimodal conversational AI research.\\nLink: http://arxiv.org/abs/2205.06907v1\\nPDF: http://arxiv.org/pdf/2205.06907v1\\n--------------------------------------------------------------------------------\\nTitle: Towards a Multimodal Document-grounded Conversational AI System for Education\\nAuthors: Karan Taneja,Anjali Singh,Ashok K. Goel\\nPublished: 2025-04-04\\nAbstract: Multimedia learning using text and images has been shown to improve learning\\noutcomes compared to text-only instruction. But conversational AI systems in\\neducation predominantly rely on text-based interactions while multimodal\\nconversations for multimedia learning remain unexplored. Moreover, deploying\\nconversational AI in learning contexts requires grounding in reliable sources\\nand verifiability to create trust. We present MuDoC, a Multimodal\\nDocument-grounded Conversational AI system based on GPT-4o, that leverages both\\ntext and visuals from documents to generate responses interleaved with text and\\nimages. Its interface allows verification of AI generated content through\\nseamless navigation to the source. We compare MuDoC to a text-only system to\\nexplore differences in learner engagement, trust in AI system, and their\\nperformance on problem-solving tasks. Our findings indicate that both visuals\\nand verifiability of content enhance learner engagement and foster trust;\\nhowever, no significant impact in performance was observed. We draw upon\\ntheories from cognitive and learning sciences to interpret the findings and\\nderive implications, and outline future directions for the development of\\nmultimodal conversational AI systems in education.\\nLink: http://arxiv.org/abs/2504.13884v1\\nPDF: http://arxiv.org/pdf/2504.13884v1\\n--------------------------------------------------------------------------------\", name='arxiv_search', id='6d5d51b1-d0c6-4b03-a0f5-03aef8120dbb', tool_call_id='46d7af1f-d1d4-49b5-afb1-54ea0d9fd75c'), AIMessage(content=\"Here are the latest research findings on multimodal AI:\\n\\n*   **Industrial CAD Modeling:** Research from March 2025 explores AI-driven multimodal interfaces to improve 3D CAD modeling workflows and user experience. It focuses on integrating multimodal AI, addressing challenges, and exploring future directions like Bayesian workflow inference and collaborative AI.\\n*   **Medicine:** A November 2024 review found that multimodal AI models in medicine consistently outperform single-modality models, with an average improvement of 6.2% in AUC. However, challenges remain in data coordination and handling diverse datasets.\\n*   **Brain Simulation:** Research from August 2022 suggests that multimodal foundation models are more effective at simulating the human brain's multimodal signal processing compared to unimodal models.\\n*   **Conversational AI:** A May 2022 survey provides a comprehensive overview of multimodal conversational AI, outlining key research areas such as representation, fusion, alignment, translation, and co-learning. It identifies co-learning as a promising future direction.\\n*   **Education:** An April 2025 paper introduces MuDoC, a multimodal document-grounded conversational AI system for education. This system uses both text and visuals from documents to generate responses and allows users to verify AI-generated content, which has been shown to enhance learner engagement and trust.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--e60c0f9b-42c6-4438-808d-0aaef0d1403c-0', usage_metadata={'input_tokens': 3839, 'output_tokens': 764, 'total_tokens': 4603, 'input_token_details': {'cache_read': 1843}, 'output_token_details': {'reasoning': 486}}), HumanMessage(content='save this info in a text file', additional_kwargs={}, response_metadata={}, id='7b6adaca-e2fe-4f25-a83a-2b3f077bce35'), AIMessage(content='Please provide a filename for the text file.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--dfab8722-14b2-40de-ac9b-263a9b7933dd-0', usage_metadata={'input_tokens': 4126, 'output_tokens': 57, 'total_tokens': 4183, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 48}}), HumanMessage(content='multi_ai.txt', additional_kwargs={}, response_metadata={}, id='f11af903-977a-4799-abdb-b676405e6393')]}}\n",
      "\n",
      "\n",
      "stream_mode: updates\n",
      "content: {'planner_node': {'messages': AIMessage(content='', additional_kwargs={'function_call': {'name': 'write_file', 'arguments': '{\"file_path\": \"multi_ai.txt\", \"text\": \"Here are the latest research findings on multimodal AI:\\\\n\\\\n*   **Industrial CAD Modeling:** Research from March 2025 explores AI-driven multimodal interfaces to improve 3D CAD modeling workflows and user experience. It focuses on integrating multimodal AI, addressing challenges, and exploring future directions like Bayesian workflow inference and collaborative AI.\\\\n*   **Medicine:** A November 2024 review found that multimodal AI models in medicine consistently outperform single-modality models, with an average improvement of 6.2% in AUC. However, challenges remain in data coordination and handling diverse datasets.\\\\n*   **Brain Simulation:** Research from August 2022 suggests that multimodal foundation models are more effective at simulating the human brain\\'s multimodal signal processing compared to unimodal models.\\\\n*   **Conversational AI:** A May 2022 survey provides a comprehensive overview of multimodal conversational AI, outlining key research areas such as representation, fusion, alignment, translation, and co-learning. It identifies co-learning as a promising future direction.\\\\n*   **Education:** An April 2025 paper introduces MuDoC, a multimodal document-grounded conversational AI system for education. This system uses both text and visuals from documents to generate responses and allows users to verify AI-generated content, which has been shown to enhance learner engagement and trust.\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--e7835c2d-ab3f-4ad5-9c8b-a276364c284b-0', tool_calls=[{'name': 'write_file', 'args': {'file_path': 'multi_ai.txt', 'text': \"Here are the latest research findings on multimodal AI:\\n\\n*   **Industrial CAD Modeling:** Research from March 2025 explores AI-driven multimodal interfaces to improve 3D CAD modeling workflows and user experience. It focuses on integrating multimodal AI, addressing challenges, and exploring future directions like Bayesian workflow inference and collaborative AI.\\n*   **Medicine:** A November 2024 review found that multimodal AI models in medicine consistently outperform single-modality models, with an average improvement of 6.2% in AUC. However, challenges remain in data coordination and handling diverse datasets.\\n*   **Brain Simulation:** Research from August 2022 suggests that multimodal foundation models are more effective at simulating the human brain's multimodal signal processing compared to unimodal models.\\n*   **Conversational AI:** A May 2022 survey provides a comprehensive overview of multimodal conversational AI, outlining key research areas such as representation, fusion, alignment, translation, and co-learning. It identifies co-learning as a promising future direction.\\n*   **Education:** An April 2025 paper introduces MuDoC, a multimodal document-grounded conversational AI system for education. This system uses both text and visuals from documents to generate responses and allows users to verify AI-generated content, which has been shown to enhance learner engagement and trust.\"}, 'id': 'b4de68fa-1dd6-431e-ae1f-4516ae8f7514', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4142, 'output_tokens': 355, 'total_tokens': 4497, 'input_token_details': {'cache_read': 3700}, 'output_token_details': {'reasoning': 51}})}}\n",
      "\n",
      "\n",
      "stream_mode: updates\n",
      "content: {'__interrupt__': (Interrupt(value=[{'action_request': {'action': 'write_file', 'args': {'file_path': 'multi_ai.txt', 'text': \"Here are the latest research findings on multimodal AI:\\n\\n*   **Industrial CAD Modeling:** Research from March 2025 explores AI-driven multimodal interfaces to improve 3D CAD modeling workflows and user experience. It focuses on integrating multimodal AI, addressing challenges, and exploring future directions like Bayesian workflow inference and collaborative AI.\\n*   **Medicine:** A November 2024 review found that multimodal AI models in medicine consistently outperform single-modality models, with an average improvement of 6.2% in AUC. However, challenges remain in data coordination and handling diverse datasets.\\n*   **Brain Simulation:** Research from August 2022 suggests that multimodal foundation models are more effective at simulating the human brain's multimodal signal processing compared to unimodal models.\\n*   **Conversational AI:** A May 2022 survey provides a comprehensive overview of multimodal conversational AI, outlining key research areas such as representation, fusion, alignment, translation, and co-learning. It identifies co-learning as a promising future direction.\\n*   **Education:** An April 2025 paper introduces MuDoC, a multimodal document-grounded conversational AI system for education. This system uses both text and visuals from documents to generate responses and allows users to verify AI-generated content, which has been shown to enhance learner engagement and trust.\"}}, 'config': {'allow_accept': True, 'allow_edit': True, 'allow_respond': True}, 'description': 'Please review the tool call'}], id='5d10d85582ecbc1fc2e1090dfc687584'),)}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for stream_mode,chunk in graph.stream(\n",
    "    initial_state,\n",
    "    stream_mode=[\"updates\", \"custom\"],\n",
    "    config=config\n",
    "):\n",
    "    print(f\"stream_mode: {stream_mode}\")\n",
    "    print(f\"content: {chunk}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23044bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "chunk =  graph.invoke(\n",
    "    initial_state,\n",
    "    config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a466ea65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3998"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_approximately(chunk['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad9c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\": \"Pandas\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://pypi.org/project/pandas/\", \"title\": \"pandas · PyPI\", \"content\": \"pandas is a Python package that provides fast, flexible, and expressive data structures designed to make working with \\\"relational\\\" or \\\"labeled\\\" data both easy\", \"score\": 0.8235701, \"raw_content\": null}, {\"url\": \"https://en.wikipedia.org/wiki/Giant_panda\", \"title\": \"Giant panda - Wikipedia\", \"content\": \"The giant panda (Ailuropoda melanoleuca), also known as the panda bear or simply panda, is a bear species endemic to China. It is characterised by its white\", \"score\": 0.7511561, \"raw_content\": null}, {\"url\": \"https://pandas.pydata.org/\", \"title\": \"pandas - Python Data Analysis Library\", \"content\": \"# pandas **pandas** is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,   built on top of the Python programming language. Install pandas now! ##### Getting started * Install pandas * Getting started * Try pandas online ##### Documentation * User guide * API reference * Contributing to pandas * Release notes ##### Community * About pandas * Ask a question * Ecosystem ##### With the support of: The full list of companies supporting *pandas* is available in the sponsors page. #### Latest version: 2.3.2 * What's new in 2.3.2 * Release date:   Aug 21, 2025 * Documentation (web) * Download source code #### Follow us #### Previous versions code code code code code\", \"score\": 0.72418857, \"raw_content\": null}], \"response_time\": 1.58, \"request_id\": \"05ed6d45-bac4-48a5-9f06-512647ac4d4b\"}\n"
     ]
    }
   ],
   "source": [
    "print(chunk['messages'][-2].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51307c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item(namespace=['users'], key='user_23', value={'hobby': 'Basketball', 'name': 'John'}, created_at='2025-08-31T19:16:11.288340+00:00', updated_at='2025-08-31T19:16:11.288340+00:00')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get((\"users\",), \"user_23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d2d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'python_repl', 'arguments': '{\"code\": \"\\\\ndef is_prime(num):\\\\n    if num < 2:\\\\n        return False\\\\n    for i in range(2, int(num**0.5) + 1):\\\\n        if num % i == 0:\\\\n            return False\\\\n    return True\\\\n\\\\nprint(is_prime(7))\\\\nprint(is_prime(10))\\\\nprint(is_prime(2))\\\\nprint(is_prime(1))\\\\nprint(is_prime(23))\\\\nprint(is_prime(49))\\\\n\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--86dd4bf9-19f6-4037-a009-305e18e2f64c-0' tool_calls=[{'name': 'python_repl', 'args': {'code': '\\ndef is_prime(num):\\n    if num < 2:\\n        return False\\n    for i in range(2, int(num**0.5) + 1):\\n        if num % i == 0:\\n            return False\\n    return True\\n\\nprint(is_prime(7))\\nprint(is_prime(10))\\nprint(is_prime(2))\\nprint(is_prime(1))\\nprint(is_prime(23))\\nprint(is_prime(49))\\n'}, 'id': '5d7091fc-3e64-45c4-a80c-f59468f70d91', 'type': 'tool_call'}] usage_metadata={'input_tokens': 2121, 'output_tokens': 190, 'total_tokens': 2311, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 55}}\n"
     ]
    }
   ],
   "source": [
    "print(chunk['messages'][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89815fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tools': {'messages': [ToolMessage(content='True\\nFalse\\nTrue\\nFalse\\nTrue\\nFalse\\n', name='python_repl', id='277e4232-a778-4516-9df3-5cbe33cccdd7', tool_call_id='5d7091fc-3e64-45c4-a80c-f59468f70d91')]}}\n",
      "\n",
      "\n",
      "{'planner_node': {'messages': AIMessage(content=['The following Python code defines a function `is_prime` that checks if a number is prime. It then tests the function with several examples:', '```python\\ndef is_prime(num):\\n    if num < 2:\\n        return False\\n    for i in range(2, int(num**0.5) + 1):\\n        if num % i == 0:\\n            return False\\n    return True\\n\\n# Test cases\\nprint(is_prime(7))   # Expected: True\\nprint(is_prime(10))  # Expected: False\\nprint(is_prime(2))   # Expected: True\\nprint(is_prime(1))   # Expected: False\\nprint(is_prime(23))  # Expected: True\\nprint(is_prime(49))  # Expected: False\\n```'], additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--8f9bc6eb-ed09-43db-8d0a-869d227d1e59-0', usage_metadata={'input_tokens': 2288, 'output_tokens': 184, 'total_tokens': 2472, 'input_token_details': {'cache_read': 0}})}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "user_input = input(\"Do you accept: ?\")\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    Command(resume=[{\"type\": user_input}]),\n",
    "    config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4dc897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='tell me about pyramid in detail and save it in a txt file', additional_kwargs={}, response_metadata={}, id='3d461e2c-da90-4ac8-a295-a358610987ae'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'wikipedia', 'arguments': '{\"query\": \"pyramid\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--49e40eaf-89df-4ccd-adbb-30b829354499-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'pyramid'}, 'id': '613a6772-c5b7-4248-a375-c8da09528e03', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1696, 'output_tokens': 63, 'total_tokens': 1759, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 49}}), ToolMessage(content='Page: Pyramid\\nSummary: A pyramid (from Ancient Greek  πυραμίς (puramís) \\'pyramid\\', from the Egyptian pir-em-us, the vertical height of the structure.) is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape, such as triangular or quadrilateral, and its surface-lines either filled or stepped.\\n\\nA pyramid has the majority of its mass closer to the ground with less mass towards the pyramidion at the apex. This is due to the gradual decrease in the cross-sectional area along the vertical axis with increasing elevation. This offers a weight distribution that allowed early civilizations to create monumental structures.Ancient civilizations in many parts of the world pioneered the building of  pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt—the latter is the only extant example of the Seven Wonders of the Ancient World.\\n\\nPage: Great Pyramid of Giza\\nSummary: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c.\\u20092600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. It is the most famous monument of the Giza pyramid complex, which is part of the UNESCO World Heritage Site \"Memphis and its Necropolis\". It is situated at the northeastern end of the line of the three main pyramids at Giza.\\nInitially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. Over time, most of the smooth white limestone casing was removed, which lowered the pyramid\\'s height to the current 138.5 metres (454.4 ft); what is seen today is the underlying core structure. The base was measured to be about 230.3 metres (755.6 ft) square, giving a volume of roughly 2.6 million cubic metres (92 million cubic feet), which includes an internal hillock. The dimensions of the pyramid were 280 royal cubits (146.7 m; 481.4 ft) high, a base length of 440 cubits (230.6 m; 756.4 ft), with a seked of \\u20605+1/2\\u2060 palms (a slope of 51°50\\'40\").\\nThe Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. The majority of the stones are not uniform in size or shape, and are only roughly dressed. The outside layers were bound together by mortar. Primarily local limestone from the Giza Plateau was used for its construction. Other blocks were imported by boat on the Nile: white limestone from Tura for the casing, and blocks of granite from Aswan, weighing up to 80 tonnes, for the \"King\\'s Chamber\" structure.\\nThere are three known chambers inside of the Great Pyramid. The lowest was cut into the bedrock, upon which the pyramid was built, but remained unfinished. The so-called Queen\\'s Chamber and King\\'s Chamber, which contain a granite sarcophagus, are above ground, within the pyramid structure. Hemiunu, Khufu\\'s vizier, is believed by some to be the architect of the Great Pyramid. Many varying scientific and alternative hypotheses attempt to explain the exact construction techniques, but, as is the case for other such structures, there is no definite consensus.\\nThe funerary complex around the pyramid consisted of two mortuary temples connected by a causeway (one close to the pyramid and one near the Nile); tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \"satellite pyramid\"; and five buried solar barques.\\n\\nPage: Egyptian pyramids\\nSummary: The Egyptian pyramids are ancient masonry structures located in Egypt.', name='wikipedia', id='daa9944e-0c2a-45a5-a325-72b58530479a', tool_call_id='613a6772-c5b7-4248-a375-c8da09528e03'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'write_file', 'arguments': '{\"file_path\": \"pyramid_info.txt\", \"text\": \"Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids\\\\u2014first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\\\n\\\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \\\\\"satellite pyramid\\\\\"; and five buried solar barques.\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--4d4e2812-b668-4cc5-b2a1-f0bff508f82a-0', tool_calls=[{'name': 'write_file', 'args': {'file_path': 'pyramid_info.txt', 'text': 'Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\n\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \"satellite pyramid\"; and five buried solar barques.'}, 'id': 'ecd0eed6-7d18-4563-a1ba-730208ff2a72', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2658, 'output_tokens': 443, 'total_tokens': 3101, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 84}}), ToolMessage(content=\"Error: ValueError('Unsupported interrupt response type: cancel')\\n Please fix your mistakes.\", name='write_file', id='62f23f96-304c-4e7d-b237-45f422b0af94', tool_call_id='ecd0eed6-7d18-4563-a1ba-730208ff2a72', status='error'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'write_file', 'arguments': '{\"file_path\": \"pyramid_info.txt\", \"text\": \"Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids\\\\u2014first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\\\n\\\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \\\\\"satellite pyramid\\\\\"; and five buried solar barques.\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--362bb830-7024-4716-bfd5-68a2c176acce-0', tool_calls=[{'name': 'write_file', 'args': {'file_path': 'pyramid_info.txt', 'text': 'Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\n\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \"satellite pyramid\"; and five buried solar barques.'}, 'id': 'd092d5b0-3d7b-4080-976d-323f0c69e8bb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3051, 'output_tokens': 466, 'total_tokens': 3517, 'input_token_details': {'cache_read': 2684}, 'output_token_details': {'reasoning': 107}})]}, next=('tools',), config={'configurable': {'thread_id': '9550', 'checkpoint_ns': '', 'checkpoint_id': '1f08324c-e5d1-69b0-8005-e79030793f77'}}, metadata={'source': 'loop', 'step': 5, 'parents': {}}, created_at='2025-08-27T09:04:15.275460+00:00', parent_config={'configurable': {'thread_id': '9550', 'checkpoint_ns': '', 'checkpoint_id': '1f08324c-c650-6bb5-8004-b4fd6ac11b02'}}, tasks=(PregelTask(id='443e2efb-43ee-3ed3-8d0b-973e7ad4f601', name='tools', path=('__pregel_pull', 'tools'), error=None, interrupts=(Interrupt(value=[{'action_request': {'action': 'write_file', 'args': {'file_path': 'pyramid_info.txt', 'text': 'Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\n\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \"satellite pyramid\"; and five buried solar barques.'}}, 'config': {'allow_accept': True, 'allow_edit': True, 'allow_respond': True}, 'description': 'Please review the tool call'}], id='203e8b26e132ac7a67598b4a48c3da77'),), state=None, result=None),), interrupts=(Interrupt(value=[{'action_request': {'action': 'write_file', 'args': {'file_path': 'pyramid_info.txt', 'text': 'Pyramid: A pyramid is a structure whose visible surfaces are triangular in broad outline and converge toward the top, making the appearance roughly a pyramid in the geometric sense. The base of a pyramid can be of any polygon shape. Ancient civilizations in many parts of the world pioneered the building of pyramids. The largest pyramid by volume is the Mesoamerican Great Pyramid of Cholula, in the Mexican state of Puebla. For millennia, the largest structures on Earth were pyramids—first the Red Pyramid in the Dashur Necropolis and then the Great Pyramid of Khufu, both in Egypt.\\n\\nGreat Pyramid of Giza: The Great Pyramid of Giza is the largest Egyptian pyramid. It served as the tomb of pharaoh Khufu, who ruled during the Fourth Dynasty of the Old Kingdom. Built c. 2600 BC, over a period of about 26 years, the pyramid is the oldest of the Seven Wonders of the Ancient World, and the only wonder that has remained largely intact. Initially standing at 146.6 metres (481 feet), the Great Pyramid was the world\\'s tallest human-made structure for more than 3,800 years. The Great Pyramid was built by quarrying an estimated 2.3 million large blocks, weighing 6 million tonnes in total. There are three known chambers inside of the Great Pyramid. The funerary complex around the pyramid consisted of two mortuary temples connected by a causeway; tombs for the immediate family and court of Khufu, including three smaller pyramids for Khufu\\'s wives; an even smaller \"satellite pyramid\"; and five buried solar barques.'}}, 'config': {'allow_accept': True, 'allow_edit': True, 'allow_respond': True}, 'description': 'Please review the tool call'}], id='203e8b26e132ac7a67598b4a48c3da77'),))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f04ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=MvNdgmM7uyc\", add_video_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6812bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "ytt_api = YouTubeTranscriptApi()\n",
    "transcript = ytt_api.fetch(\"MvNdgmM7uyc\").to_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d02b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi this is Lan from Lang chain I want totalk about using Lang graph for codeGeneration Um so co- generation is oneof the really interesting applicationsof llms like we've seen projects likeGitHub co-pilot become extremely popularum and a few weeks ago a paper came outum by the folks at codium AI calledAlpha codium and this was really coolpaper in particular because itintroduced this ideaof doing code generation using what youcan think of as like flow engineering soinstead of just like an llm a codingprompt like solve this problem and asolution what it does is it generates aset of solutions ranks them so that'sfine that's like kind of standard likekind of prompt response style flow butwhat it does here that I want to drawyour attention to is if it actuallytests that code in a few different wayson public tests and on AI generatedtests and the key point is this itactually iterates and tries to improvethe solution based upon those testresults so that was reallyinteresting and a tweet came out bykarpathy on this theme which kind ofmentions hey this idea of flowengineering is a really nice Paradigmmoving away from just like naive promptanswer toflow where you can build up an answeritely over time usingtesting so it's a really nice idea andwhat's kind of cool is a few weeks agowe introduced Lang graph as a way tobuild kind of arbitrary graphs which canrepresent different kinds of flows andI've done some videos on this previouslytalking about Lang graph or things likerag where like you can do retrieval andthen you can do like a retrieval qualitycheck like grade the documents ifthey're not good you can like try toretrieve again or you can like do a websearch or something but it's a way torepresent arbitrary kind of logicalflows withllms in a lot of the same way we do withagents but the benefit of graphs is thatyou can outline a flow that's a littlebit more it's kind of like an agent withguardrails it's like you define thesteps in a very particular order andevery time you run the graph it justexecutes in thatorder so what I want to do is I want totry to implement some of the ideas fromalpha codium using L graph and we'regoing to do that right now so inparticular let's say we want to answercoding questions about some part of theLang chain documentation and for thisI'm going to choose the L chainexpression language docs so it's asubset of our docs it's around 60,000tokens and it focuses only on line chainexpression language which is basically away you can represent chains usinginline chain and we'll talk about thatin a littlebit but I want to do a few simple thingsso I want to have one what we're goingto call a node in our graph that takes aquestion and outputs an answer usingLang chain expression language docs as areference and then with that answer Iwant to be able to parse out componentsso given the answer I want to be able toparse out like the Preamble what is thisanswering the import specifically andthen the code and to do this I want touse some like a pedantic object so it'slike very nicelyformatted if I have that I can reallyeasily Implement tests for things likecheck to make sure the Imports workcheck to make sure the code executes andif either of those fail I can loop backto my generation node and say Hey tryagain here's the error Trace so againwhat they're doing in Al codium is waymore sophisticated I don't mean tosuggest we're iing imple M this as is umthis actually works on like a bunch ofpublic coding challenges it actually hastests um for each question that are bothadd and publicly available so againwe're doing something much simpler but Iwant to show how you can Implement thesekinds of ideas and you can make itarbitrarily complex if youwant so I'm going to copy over some codeinto a notebook that I have running andall I've done is I've just done some pipinstalls and I've BAS to find a fewenvironment variables for Lang Smithwhich we'll see later is prettyuseful and I'm going to call thisdocs so this is where I'm going toingest the docs related to Langexpression language and I'm going tokick off uh this right now so that'srunning so again this is using a URLloader grab all the docs sort them andclean them a little bit and here we goso here we go these are all the docsrelated to Lang and expression languageit's around 60,000 token tokens I'vemeasured it in the past so there's ourdocs now I I want to show you somethingthat's very useful um I'll call it tooluse um with open ey models and and otherLMS have similar functionality but Iwant to show you something that's reallyuseful um what I'm going to do here isshow how to build a chain that willoutput remember we talked about in ourdiagram we want three things for everysolution we want a preamble we want wantImports we want code as a structuredobject that we can like work withindividually I'll show you right herehow to do that so we're doing is we'reimporting um uh from pantic this basemodel and field and we're defining adata model for our output so I want aprefix which is just like the plainlanguage solution like here's the setupto the problem the import statement andthe code I want those as three distinctthings that I can work withlater I'll use dpd4 uh1 25 say 128context window model um and what I'mgoing to do is I'm going to take thisthis data model I'm going to turn itinto a tool and I'm going to bind it tomy model and so basically What'sHappening Here is it's going to alwaysperform a function call to attempt toOutput in this format I specify herethat's all it's happening I Define aprompt that says here's all the L cellthey're Lang CH expression languagepronounced or or substituted as LChere's all LCL docs answer the questionsstructure your output in a few ways butwhat's cool is we're always growing thatfunction call to basically try to Outputa pantic object so there we go nowwhat's nice is I can just invoke thiswith a question so let's just trythat so I'm going to sayquestion and I'm going to say how tocreate a rag chain in NLC we want torunum okay this needs to be a dict there wego boom so that'srunning now this is actually just we cansee right here we passed in all thosedocs that we previously loaded so it'slike 60,000 tokens of context and againyou think about newer long context llmslike Gemini that becomes more morefeasible to do do things like this takelike a whole a whole code base a wholeset of documentation load it and juststuff it into a model and have it sayhey answer questions about this that'sstill running now the latency isdefinitely higher because it's very verylarge context but that's fine we have alittle bit of time and we can go over toLang Smith Al this is running and have alook so we can see here was our promptokay so there you go look at this 63,000tokens you can see it's a lot of contextum that's fine and we can actually seeit all here so it's in Langs Smith um wedon't want to scroll through all thatmess but you can see we've asked aquestion we're grounding the response inall this L cell docs and we're going tohopefully output the response as apedantic object that we can play with solet's just see and okay nice it's doneso you can see our object here has aprefix um and it actually has um it'salso going to have our Imports here aswell we can actually can see that inlsmith uh the answer is going to be hereand there you go see your Imports yourcode and your prefix and these can allbe extracted from that object uh reallyeasily um so it's basically a list andit's a pantic object code and you canextract each one just like Co you knowanswer. prefix answer do uh whatever ourvariables or whatever our keys areanswer. Imports answer. code so that'sgreat so that just shows you how tooluse Works um and how we can get thestructured output out of our generationnodenow what I'm going to do here is nowthat we've established that we can dothat I'm going to start setting up ourgraph and what I'm going to do is firstI'm going to find our graph state sothis is just going to be a dictionarywhich contains things relevant to ourproblem it'll contain our code solutionit'll contain any errors and that's allwe're going toneed and here is where this is all thecode related to My Graph and we're goingto walk through this so don't worry toomuch but I just want to kind of get thisall hereso here's our code now if we go up theway to think about this is simply thisum I want to go back to my diagram hereso every node in our graph just has acorresponding function and that functionmodifies the state in some way so what'shappening is our generation node isgoing to be working with question anditeration those are the parts of statethat we want as like inputs you can seeit kind of maps to here you havequestion and you have iteration justcounts how many times you've tried thiswe'll see why that's interestinglater um this is exactly what we sawbefore data model llm tooluse all the same stuff right templatenow here's where it'sinteresting if our state contains anerror this error key what that means iswe've fed back from some of our testsand we have an error that's already beengenerated so we're retrying here's whyinteresting if we'reretrying what we're going to do isappend our prompt just like we saw abovewe're going to add something to ourprompt that says hey you tried thisbefore here was your solution we savedthat as generation key um and in ourstates you can see it's right here codesolution generation here is your errorplease retry to answer this so it's kindof like inducing a reflection based onyour priorgeneration anderror and retry so that's a veryimportant point because basically givesus that feedback from if there's amistake and either the Imports or theexecutions we're feeding that back togeneration and generation is going toretry with that information present sothat's that's all that's happening thereum and we're basically adding that tothe prompt um and we're then invokingthe chain with that error and then we'regetting a new code solution so againthat's if errors in our state dick if itisn't then we're going to go ahead andgenerate our solution just like we didabove same thing soeasy um one little thing is every timewe return the the basically we're goingto rewrite that output to the statewe're going to increment our iterationsand say Here's how many times we'vetried to answer this question that'sreally it and you can see that's all wedo return the generation return thequestion return the number of iterationseasy now here's what's kind of nice wetalked about having these two importchecks the check for imports to checkfor execution let's our check importnode just going to be really simple wehave oursolution from the solution we can getthe Imports out just like we showedabove this code solution Imports is fromour pantic object um I'll move it overso you can kind of see so a panticobject has Imports we can get theImports and all we do is just attempt toexecute the Imports if it fails we alerthey import check failed and here's thekey point we're just going to create anew key uh error in our dict identifyingthat hey there's an error present umsomething failed here and you'll seewe're going to use that later now oneother little trick if there was a priorerror in our state we're just going topend it so we do want to kind ofmaintain that um if there's like anaccumulation of errors as we runmultiple iterations we want to keepaccumulating them so we don't likerevert and make the same mistake wealready did on a future iteration sowe're going to maintain our set ofErrors now if there's no error here thenwe're going to rewrite none so we'regoing to say we're good keep going uhand basically the same thing with codeexecution right in that case we're justextracting our code and our Imports wecreate a code block of imports Plus Codetry to execute it again if it failswrite our error and append all priorerrors if it doesn't return none that'sit that's all you really need to knownow here note that we're going to do twokind of gates so we want to know if dideither of those tests fail and again allwe need to do is we can uh grab ourerror and remember if there is no errorthen if error is none keep going so herewe're at the code execution likedecision point so do you want to go tocode execution or do you want to likerevert back and retry so you can seehere if there's no error when we get tothe this point um then because we'vedone our import check if there's noerror there keep going go to codeexecution we can see we return this nodewe want to go to um and if there is anerror we can say hey return to thegenerate node so really what thesefunctions do so these are conditionaledges what these do is they do some kindof conditional check based upon like ouroutput state so again if there's anerror or not if there's no error ittells you go to this node if there is anerror it tells you go back to generatenode that's it same deal with decidingto finish again if there's no error andnow here's the iteration thing for thesake of Simplicity what I say is give itthree tries I don't want it to runarbitrarily long uh if there's no erroror if you try three times just endthat's it uh otherwise go back togenerate so again same kind of thingdecide to finished based upon um yeahbased upon whether or not there's anerror in our code execution or notthat's really it that's all we're doingso we can go down we already grabbed allthis now here is where we actuallyDefine our what we call ourworkflow um and so this is actuallywhere we defined all our edges and nodesas these functions and here's just wherewe kind of stitch them all together umso it's actually pretty straightforwardit just follows exactly like the diagramwe showed above um we like we'rebasically adding all of our nodes andwe're building our graph following thediagram that we show so we can go backto the diagram so like you can kind offollow along right set your entry pointit's generate add an edge generate checkcode Imports now our conditional Edge umso if we're going to decide to checkcode execution that was our functionhere right hereso if um basically depending on theoutput here we can decide the next nodeto go to so um if the output of thefunction says check codeexecution we go to that node if theoutput says go to generate we go back togenerate so these are where you specifythe logic of the next node you want togo to and same here so that's all we docompile it done and just map to thisdiagram um kind of like one to one sothat's actually prettystraightforward and there's just onelittle thing we now need to do uh we aregoing to goahead and try aquestion so here's a question I've I'verun a bunch of these tests already thisis a question that seems kind of randombut it's like we actually built a NEvalve set and so it's a question that wewe've sound that there's some problemswith so I want to show you why this ispretty cool I'm passing it text Key Foodin my prompt I want to process it withsome function process text how do I dothis using uh line transpressionlanguage so it's a weird question butyou'll see why it's kind of fun in ashort in a little bit and what I'm goingto do is I'm just going to run my graphso what we can see because we print outwhat happens at every step we're can tokind of follow along and see what'shappeninghere um so it's going to generatesolution now we can see this may take alittle bit because it's the same kind oflong context generation that we sawpreviously so this is now running we cango to Lang Smith and we can actuallyjust check this Lang graph and we cansee okay so it's loading up and we're atgenerate so it's actually doing thisgeneration this is still pending here isall our input docs so you can see thatum that uh you know we passed this verylarge context to our LM uh so that'scool okay so here's this is interestingso what's happening is it's goingthrough some checks so um the codeimport check worked decide to check codeexecution a decision testing codeexecutionhere's an interesting one code blockcheckfailed um decision retry so it'sactually doing aregenerationso okay let's see it looks like it cameto an answer um let's actually go andlook at what happened in our Lang graphto kind of understand what happened sowhat happened if we look at theum let's look at when weattempted yeah exactly so here let meactually pull up the errorhereum here was ourresponse um and what I want to show youis the error that we appended to ourpromptumand we can actually make this a bitfaster we canscroll this is the Crux of what I wantto show you um okay here it is so what'scool is our initial attempt to solvethis problem introduced an error therewas an execution error it unsupportedOpera for types dict and string sobasically it did something wrong and wepassed that in the prompt to the llmwhen it performs this retry so the ourinitial solution was here and it had a acoding error as noted but here you cansee we provide that error and we sayplease try to reans this structure youknow like the same instructions beforehere was uh here was the the questionand we can see okay so this is actuallythe test of code execution which nowworks we can see previously when wetried this uh it fails and this was theerror that error was passed along in theprompt like we just saw the new the newtest indeed Works our final solution isfunctionalcode that's it so you can kind of getsome intuition for the fact that whenyou have this retry Lube you can recoverfrom errors using a little bit ofreflection that's really the big idea umand again you get your answer outhere um and so there's a bunch of keysand we don't necessarily I'll show youquickly uh keys and then we can justlook at the generationkey umcool and it's going to be a list solet's just break it out so there it isthere's our code object we can seeprefix okay so there's the prefiximport uh and let's try code and heylet's just convince ourselves thisactually works so we can just run exeImports that worksexexec the code and this should work it'sdoing something there it tells a jokegreat um so this is pretty cool itinitially when to try to answer thisquestion produce an error and it thenretry by passing that error back to thecontext just like we outlined in ourgraph and on the second try it gets itcorrect so that's nice it's a goodexample of how you can do this feedbackand and reflection stuff now we actuallyhave done quite a bit more work on thisso I built an eval set of 20 Questionsrelated to Lang and expression languageand evaluated them all uh using thisapproach relative to not using Langgraph and here's the results I want tokind of draw your attention to thisbecause it's a pretty interesting resultfor the import check without Lang graphversus with Lang graph it's about thesame so Imports weren't really a problembefore this like retry reflection stuffImports were okay on oural set of 20questions I should make a note weactually ran this uh this is showingstandard errors we ran this four timesand so I basically accumulate theresults I compute standard errors youcan see that there's there's some degreeof statistical reasonable inness tothese results um in any case importchecks were were kind of fine without itbut here's a big difference there's abig difference in our code execution uhper performance with and without landgraph so before land graph if you justtry like kind of single shot answergeneration a lot of the times this waslike a 55% success rate many of thecases we saw code execution fail butwith Lang graph with this kind of retryand reflection stuff we actually can seethat the the success rate goes up toaround I believe it was 80% so it'saround like a almost a 50% Improvementin performance um with and without usingL graph so that was actually reallyimpressive and and it just shows thepower of like a very simple idea umattempting code generation with thesekinds of like just very simple checksand reflection can significantly bump upyour performance and again the alphacodium paper shows this in like a verysophisticated context but what's cool isthis is like a very simple idea you canimp Implement yourself in not much timeum and we have this all available as anotebook you can run this on any pieceof code you want so just take whateverdocuments want here Plum them in and youcan run this and you can test this outfor yourself but I've been reallyimpressed I think it's pretty cool umand in general I think lra's a reallynice way to build these kind of likereflective or self-reflectiveapplications where you can build thesefeedback loops to you can do a check ifthe check fails try again with that Fewith that uh feedbackpresent um in the retry and I'll justshow you we have a Blog coming out I'mnot sure there's anything else in thatblog I haven't already showed youum yeah not nothing really to highlightthis was our results again um this ismaybe a little bit clearer to see um butagain pretty significant Improvement inperformance simple idea uh I definitelyencourage you to experiment with this umand of course all this code will beavailable for you so um uh you know feelfree to experiment and let us know howit goes thankyou\n"
     ]
    }
   ],
   "source": [
    "output = \"\"\n",
    "for t in transcript:\n",
    "    output += t['text']\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83653772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.config import get_store\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "store = InMemoryStore() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88a447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item(namespace=['istyle'], key='use3', value={'tone': 'casual', 'format': '', 'language': ''}, created_at='2025-08-31T15:59:33.261498+00:00', updated_at='2025-08-31T15:59:33.261498+00:00')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get((\"istyle\",), \"use3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespaces in store\n",
    "(\"users\", user_id)         # user profile\n",
    "(\"preferences\", user_id)   # preferences\n",
    "(\"knowledge\", user_id)     # facts\n",
    "(\"tasks\", user_id)         # goals\n",
    "(\"interactions\", user_id)  # style/history\n",
    "(\"domain\", user_id)        # domain-specific info\n",
    "(\"world\", user_id)         # external/tool context\n",
    "(\"sessions\", user_id)      # session summaries\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
